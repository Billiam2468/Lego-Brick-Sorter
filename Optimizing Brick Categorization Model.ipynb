{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9aebe0",
   "metadata": {},
   "source": [
    "# This is the notebook for optimizing the model for categorizing our Lego bricks. Categories will be defined by Tom Alphin's Lego Brick Labels (v39)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5bc457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 15:05:36.244954: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import os  \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e93b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(617.16956, shape=(), dtype=float32)\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c822fadc",
   "metadata": {},
   "source": [
    "# Comparing Transfer Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c645824f",
   "metadata": {},
   "source": [
    "We will be using transfer learning techniques on Keras pre-trained models to create our categorization model. We will first need to test out all of the different Keras Applications and discover which one will best work for our needs.\n",
    "\n",
    "\n",
    "\n",
    "**Training Dataset:**\n",
    "\n",
    "We will be using the 447 class training data set referenced in this paper:https://www.iccs-meeting.org/archive/iccs2022/papers/133520608.pdf\n",
    "\n",
    "**Testing:**\n",
    "\n",
    "We will be selecting 20 random classes of Lego brick to train our models on. Models will then be compared with each other to determine what will best work for our purposes\n",
    "\n",
    "**Data Augmentation:**\n",
    "\n",
    "In order to save time for finding an optimal model, no data augmentation other than resizing our images (without preserving aspect ratio) to match the input of the models will be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663e295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of models we will test\n",
    "model_dict = {}\n",
    "model_dict[\"VGG16\"] = tf.keras.applications.VGG16\n",
    "model_dict[\"VGG19\"] = tf.keras.applications.VGG19\n",
    "model_dict[\"ResNet101V2\"] = tf.keras.applications.ResNet101V2\n",
    "# model_dict[\"ResNet152\"] = tf.keras.applications.ResNet152\n",
    "model_dict[\"ResNet152V2\"] = tf.keras.applications.ResNet152V2\n",
    "model_dict[\"ResNet50\"] = tf.keras.applications.ResNet50\n",
    "model_dict[\"ResNet50V2\"] = tf.keras.applications.ResNet50V2\n",
    "model_dict[\"InceptionResNetV2\"] = tf.keras.applications.InceptionResNetV2\n",
    "model_dict[\"InceptionV3\"] = tf.keras.applications.InceptionV3\n",
    "model_dict[\"MobileNet\"] = tf.keras.applications.MobileNet\n",
    "model_dict[\"MobileNetV2\"] = tf.keras.applications.MobileNetV2\n",
    "# model_dict[\"MobileNetV3\"] = tf.keras.applications.MobileNetV3 <- weird exceptions\n",
    "model_dict[\"Xception\"] = tf.keras.applications.Xception\n",
    "model_dict[\"DenseNet121\"] = tf.keras.applications.DenseNet121\n",
    "model_dict[\"DenseNet169\"] = tf.keras.applications.DenseNet169\n",
    "model_dict[\"DenseNet201\"] = tf.keras.applications.DenseNet201\n",
    "model_dict[\"EfficientNetV2S\"] = tf.keras.applications.EfficientNetV2S\n",
    "model_dict[\"EfficientNetV2M\"] = tf.keras.applications.EfficientNetV2M\n",
    "model_dict[\"EfficientNetV2L\"] = tf.keras.applications.EfficientNetV2L\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7063c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: VGG16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                         | 1/17 [00:00<00:04,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: VGG19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████▏                                      | 2/17 [00:00<00:04,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ResNet101V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▊                                    | 3/17 [00:02<00:13,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ResNet152V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████▎                                 | 4/17 [00:05<00:21,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ResNet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████▉                               | 5/17 [00:05<00:16,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ResNet50V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███████████████▌                            | 6/17 [00:06<00:13,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: InceptionResNetV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|██████████████████                          | 7/17 [00:10<00:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: InceptionV3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████████████████████▋                       | 8/17 [00:11<00:16,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: MobileNet\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████▎                    | 9/17 [00:12<00:10,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: MobileNetV2\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████████████████████████▎                 | 10/17 [00:13<00:08,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Xception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████▊               | 11/17 [00:14<00:06,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: DenseNet121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████▎            | 12/17 [00:15<00:06,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: DenseNet169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████▉          | 13/17 [00:18<00:06,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: DenseNet201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████▍       | 14/17 [00:21<00:06,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: EfficientNetV2S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████▉     | 15/17 [00:23<00:04,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: EfficientNetV2M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|████████████████████████████████████████▍  | 16/17 [00:26<00:02,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: EfficientNetV2L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 17/17 [00:32<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load in our preprocessed models into a dictionary\n",
    "preprocessed_models = {}\n",
    "for model_name, model in tqdm(model_dict.items()):\n",
    "    print(\"Processing: \" + model_name)\n",
    "    preprocessed_models[model_name] = model(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9080c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: VGG16\n",
      "processing: VGG19\n",
      "processing: ResNet101V2\n",
      "processing: ResNet152V2\n",
      "processing: ResNet50\n",
      "processing: ResNet50V2\n",
      "processing: InceptionResNetV2\n",
      "processing: InceptionV3\n",
      "processing: MobileNet\n",
      "processing: MobileNetV2\n",
      "processing: Xception\n",
      "processing: DenseNet121\n",
      "processing: DenseNet169\n",
      "processing: DenseNet201\n",
      "processing: EfficientNetV2S\n",
      "processing: EfficientNetV2M\n",
      "processing: EfficientNetV2L\n"
     ]
    }
   ],
   "source": [
    "# Process our models\n",
    "# (delete the last layer, make all remaining layers untrainable, and add our own trainable layer)\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "processed_models = {}\n",
    "for name, pre_model in preprocessed_models.items():\n",
    "    print(\"processing: \" + name)\n",
    "    # Create our empty model (look up sequential vs functional)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Add all layers from our pre-trained model (last layer already deleted from include_top=False)\n",
    "    model.add(pre_model)\n",
    "\n",
    "    # Make all remaining layers untrainable and add our last trainable layer\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # model.summary()\n",
    "        \n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(NUM_CLASSES))\n",
    "    \n",
    "    # Loss and optimizer functions\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optim = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Might try mean average precision for metric because we are categorizing so many classes\n",
    "    metrics = [\"accuracy\"]\n",
    "    \n",
    "    # Used to do accuracy but someone online recommended MAP\n",
    "    # https://www.reddit.com/r/learnmachinelearning/comments/xpyv8j/data_set_for_lego_image_classification_800000/\n",
    "    # metrics = [\"accuracy\"]\n",
    "\n",
    "    # Compile our model\n",
    "    model.compile(optimizer=optim, loss=loss, metrics=metrics)\n",
    "    \n",
    "    # Add model to our dict\n",
    "    processed_models[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "952d7650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: VGG16\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, None, None, 512)   14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,717,253\n",
      "Trainable params: 2,565\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 54s - loss: 0.5436 - accuracy: 0.8139 - val_loss: 0.2961 - val_accuracy: 0.8946 - 54s/epoch - 26ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 53s - loss: 0.2363 - accuracy: 0.9126 - val_loss: 0.2062 - val_accuracy: 0.9300 - 53s/epoch - 25ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 53s - loss: 0.1860 - accuracy: 0.9358 - val_loss: 0.1961 - val_accuracy: 0.9291 - 53s/epoch - 25ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 53s - loss: 0.1663 - accuracy: 0.9410 - val_loss: 0.1741 - val_accuracy: 0.9388 - 53s/epoch - 25ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 52s - loss: 0.1477 - accuracy: 0.9482 - val_loss: 0.1675 - val_accuracy: 0.9442 - 52s/epoch - 25ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 52s - loss: 0.1374 - accuracy: 0.9482 - val_loss: 0.1960 - val_accuracy: 0.9371 - 52s/epoch - 25ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 52s - loss: 0.1280 - accuracy: 0.9543 - val_loss: 0.2558 - val_accuracy: 0.9170 - 52s/epoch - 25ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 52s - loss: 0.1288 - accuracy: 0.9549 - val_loss: 0.2806 - val_accuracy: 0.9076 - 52s/epoch - 25ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 52s - loss: 0.1196 - accuracy: 0.9560 - val_loss: 0.1994 - val_accuracy: 0.9396 - 52s/epoch - 25ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 52s - loss: 0.1168 - accuracy: 0.9572 - val_loss: 0.2132 - val_accuracy: 0.9368 - 52s/epoch - 24ms/step\n",
      "Epoch 10: early stopping\n",
      "Performance ofVGG16: \n",
      "529/529 - 9s - loss: 0.2347 - accuracy: 0.9281 - 9s/epoch - 17ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: VGG19\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, None, None, 512)   20024384  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,026,949\n",
      "Trainable params: 2,565\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 64s - loss: 0.5457 - accuracy: 0.8127 - val_loss: 0.2555 - val_accuracy: 0.9062 - 64s/epoch - 30ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 63s - loss: 0.2070 - accuracy: 0.9266 - val_loss: 0.2226 - val_accuracy: 0.9238 - 63s/epoch - 30ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 63s - loss: 0.1644 - accuracy: 0.9402 - val_loss: 0.1826 - val_accuracy: 0.9382 - 63s/epoch - 30ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 63s - loss: 0.1458 - accuracy: 0.9467 - val_loss: 0.2009 - val_accuracy: 0.9357 - 63s/epoch - 30ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 63s - loss: 0.1320 - accuracy: 0.9533 - val_loss: 0.1806 - val_accuracy: 0.9433 - 63s/epoch - 30ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 63s - loss: 0.1252 - accuracy: 0.9547 - val_loss: 0.3769 - val_accuracy: 0.8713 - 63s/epoch - 30ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 63s - loss: 0.1165 - accuracy: 0.9604 - val_loss: 0.1612 - val_accuracy: 0.9498 - 63s/epoch - 30ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 63s - loss: 0.1128 - accuracy: 0.9589 - val_loss: 0.3555 - val_accuracy: 0.8906 - 63s/epoch - 30ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 63s - loss: 0.1114 - accuracy: 0.9602 - val_loss: 0.1921 - val_accuracy: 0.9439 - 63s/epoch - 30ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 64s - loss: 0.1004 - accuracy: 0.9646 - val_loss: 0.1831 - val_accuracy: 0.9504 - 64s/epoch - 30ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 63s - loss: 0.0998 - accuracy: 0.9642 - val_loss: 0.2056 - val_accuracy: 0.9416 - 63s/epoch - 30ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 63s - loss: 0.0983 - accuracy: 0.9647 - val_loss: 0.2603 - val_accuracy: 0.9362 - 63s/epoch - 30ms/step\n",
      "Epoch 12: early stopping\n",
      "Performance ofVGG19: \n",
      "529/529 - 11s - loss: 0.2529 - accuracy: 0.9305 - 11s/epoch - 21ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: ResNet101V2\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet101v2 (Functional)    (None, None, None, 2048)  42626560  \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,636,805\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 42,626,560\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 61s - loss: 0.2427 - accuracy: 0.9127 - val_loss: 0.1611 - val_accuracy: 0.9410 - 61s/epoch - 29ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 56s - loss: 0.1066 - accuracy: 0.9604 - val_loss: 0.1648 - val_accuracy: 0.9334 - 56s/epoch - 27ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 57s - loss: 0.0830 - accuracy: 0.9705 - val_loss: 0.1312 - val_accuracy: 0.9467 - 57s/epoch - 27ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 57s - loss: 0.0648 - accuracy: 0.9754 - val_loss: 0.1002 - val_accuracy: 0.9617 - 57s/epoch - 27ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 57s - loss: 0.0573 - accuracy: 0.9771 - val_loss: 0.1027 - val_accuracy: 0.9660 - 57s/epoch - 27ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 57s - loss: 0.0436 - accuracy: 0.9842 - val_loss: 0.1039 - val_accuracy: 0.9657 - 57s/epoch - 27ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 57s - loss: 0.0387 - accuracy: 0.9855 - val_loss: 0.1280 - val_accuracy: 0.9558 - 57s/epoch - 27ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 57s - loss: 0.0332 - accuracy: 0.9872 - val_loss: 0.1308 - val_accuracy: 0.9626 - 57s/epoch - 27ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 56s - loss: 0.0295 - accuracy: 0.9894 - val_loss: 0.0946 - val_accuracy: 0.9697 - 56s/epoch - 27ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 57s - loss: 0.0248 - accuracy: 0.9905 - val_loss: 0.1077 - val_accuracy: 0.9680 - 57s/epoch - 27ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 57s - loss: 0.0223 - accuracy: 0.9917 - val_loss: 0.1458 - val_accuracy: 0.9603 - 57s/epoch - 27ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 57s - loss: 0.0226 - accuracy: 0.9915 - val_loss: 0.0996 - val_accuracy: 0.9685 - 57s/epoch - 27ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 57s - loss: 0.0194 - accuracy: 0.9935 - val_loss: 0.1998 - val_accuracy: 0.9374 - 57s/epoch - 27ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 57s - loss: 0.0185 - accuracy: 0.9928 - val_loss: 0.1507 - val_accuracy: 0.9632 - 57s/epoch - 27ms/step\n",
      "Epoch 14: early stopping\n",
      "Performance ofResNet101V2: \n",
      "529/529 - 10s - loss: 0.1498 - accuracy: 0.9655 - 10s/epoch - 19ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training of: ResNet152V2\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet152v2 (Functional)    (None, None, None, 2048)  58331648  \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,341,893\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 58,331,648\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 86s - loss: 0.2763 - accuracy: 0.8968 - val_loss: 0.1392 - val_accuracy: 0.9515 - 86s/epoch - 41ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 80s - loss: 0.1291 - accuracy: 0.9532 - val_loss: 0.1387 - val_accuracy: 0.9484 - 80s/epoch - 38ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 80s - loss: 0.0900 - accuracy: 0.9681 - val_loss: 0.1195 - val_accuracy: 0.9583 - 80s/epoch - 38ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 80s - loss: 0.0784 - accuracy: 0.9725 - val_loss: 0.1269 - val_accuracy: 0.9561 - 80s/epoch - 38ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 80s - loss: 0.0672 - accuracy: 0.9758 - val_loss: 0.1404 - val_accuracy: 0.9532 - 80s/epoch - 38ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 80s - loss: 0.0560 - accuracy: 0.9796 - val_loss: 0.1035 - val_accuracy: 0.9634 - 80s/epoch - 38ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 80s - loss: 0.0439 - accuracy: 0.9835 - val_loss: 0.1361 - val_accuracy: 0.9561 - 80s/epoch - 38ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 81s - loss: 0.0427 - accuracy: 0.9845 - val_loss: 0.1638 - val_accuracy: 0.9507 - 81s/epoch - 38ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 80s - loss: 0.0398 - accuracy: 0.9850 - val_loss: 0.1080 - val_accuracy: 0.9708 - 80s/epoch - 38ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 80s - loss: 0.0322 - accuracy: 0.9881 - val_loss: 0.1129 - val_accuracy: 0.9666 - 80s/epoch - 38ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 80s - loss: 0.0301 - accuracy: 0.9900 - val_loss: 0.1659 - val_accuracy: 0.9532 - 80s/epoch - 38ms/step\n",
      "Epoch 11: early stopping\n",
      "Performance ofResNet152V2: \n",
      "529/529 - 14s - loss: 0.1858 - accuracy: 0.9470 - 14s/epoch - 27ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: ResNet50\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, None, None, 2048)  23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,597,957\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 43s - loss: 0.2895 - accuracy: 0.8928 - val_loss: 0.1461 - val_accuracy: 0.9481 - 43s/epoch - 20ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 40s - loss: 0.1328 - accuracy: 0.9519 - val_loss: 0.1124 - val_accuracy: 0.9580 - 40s/epoch - 19ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 40s - loss: 0.1039 - accuracy: 0.9611 - val_loss: 0.0894 - val_accuracy: 0.9697 - 40s/epoch - 19ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 40s - loss: 0.0808 - accuracy: 0.9720 - val_loss: 0.1420 - val_accuracy: 0.9498 - 40s/epoch - 19ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 40s - loss: 0.0667 - accuracy: 0.9757 - val_loss: 0.0886 - val_accuracy: 0.9705 - 40s/epoch - 19ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 40s - loss: 0.0598 - accuracy: 0.9781 - val_loss: 0.0852 - val_accuracy: 0.9700 - 40s/epoch - 19ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 40s - loss: 0.0507 - accuracy: 0.9810 - val_loss: 0.1084 - val_accuracy: 0.9685 - 40s/epoch - 19ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 40s - loss: 0.0481 - accuracy: 0.9819 - val_loss: 0.0972 - val_accuracy: 0.9719 - 40s/epoch - 19ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 40s - loss: 0.0414 - accuracy: 0.9837 - val_loss: 0.0874 - val_accuracy: 0.9725 - 40s/epoch - 19ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 40s - loss: 0.0314 - accuracy: 0.9889 - val_loss: 0.0874 - val_accuracy: 0.9736 - 40s/epoch - 19ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 40s - loss: 0.0327 - accuracy: 0.9881 - val_loss: 0.1491 - val_accuracy: 0.9623 - 40s/epoch - 19ms/step\n",
      "Epoch 11: early stopping\n",
      "Performance ofResNet50: \n",
      "529/529 - 7s - loss: 0.1460 - accuracy: 0.9612 - 7s/epoch - 13ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: ResNet50V2\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50v2 (Functional)     (None, None, None, 2048)  23564800  \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,575,045\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 36s - loss: 0.2670 - accuracy: 0.9036 - val_loss: 0.1615 - val_accuracy: 0.9408 - 36s/epoch - 17ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 34s - loss: 0.1254 - accuracy: 0.9547 - val_loss: 0.1688 - val_accuracy: 0.9362 - 34s/epoch - 16ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 34s - loss: 0.0998 - accuracy: 0.9614 - val_loss: 0.1475 - val_accuracy: 0.9459 - 34s/epoch - 16ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 34s - loss: 0.0747 - accuracy: 0.9732 - val_loss: 0.1164 - val_accuracy: 0.9617 - 34s/epoch - 16ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 34s - loss: 0.0642 - accuracy: 0.9757 - val_loss: 0.1354 - val_accuracy: 0.9549 - 34s/epoch - 16ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 34s - loss: 0.0579 - accuracy: 0.9796 - val_loss: 0.1112 - val_accuracy: 0.9612 - 34s/epoch - 16ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 34s - loss: 0.0481 - accuracy: 0.9829 - val_loss: 0.1307 - val_accuracy: 0.9583 - 34s/epoch - 16ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 34s - loss: 0.0457 - accuracy: 0.9845 - val_loss: 0.1279 - val_accuracy: 0.9617 - 34s/epoch - 16ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 34s - loss: 0.0400 - accuracy: 0.9863 - val_loss: 0.1230 - val_accuracy: 0.9640 - 34s/epoch - 16ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 34s - loss: 0.0350 - accuracy: 0.9872 - val_loss: 0.1299 - val_accuracy: 0.9637 - 34s/epoch - 16ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 34s - loss: 0.0315 - accuracy: 0.9898 - val_loss: 0.1295 - val_accuracy: 0.9640 - 34s/epoch - 16ms/step\n",
      "Epoch 11: early stopping\n",
      "Performance ofResNet50V2: \n",
      "529/529 - 6s - loss: 0.1323 - accuracy: 0.9617 - 6s/epoch - 12ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: InceptionResNetV2\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inception_resnet_v2 (Functi  (None, None, None, 1536)  54336736 \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_6   (None, 1536)             0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 5)                 7685      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,344,421\n",
      "Trainable params: 7,685\n",
      "Non-trainable params: 54,336,736\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 81s - loss: 0.3440 - accuracy: 0.8698 - val_loss: 0.2722 - val_accuracy: 0.9039 - 81s/epoch - 38ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 74s - loss: 0.2209 - accuracy: 0.9154 - val_loss: 0.1715 - val_accuracy: 0.9385 - 74s/epoch - 35ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 73s - loss: 0.1780 - accuracy: 0.9345 - val_loss: 0.1862 - val_accuracy: 0.9376 - 73s/epoch - 34ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 73s - loss: 0.1639 - accuracy: 0.9395 - val_loss: 0.1843 - val_accuracy: 0.9382 - 73s/epoch - 34ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 73s - loss: 0.1433 - accuracy: 0.9467 - val_loss: 0.1295 - val_accuracy: 0.9575 - 73s/epoch - 34ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 73s - loss: 0.1299 - accuracy: 0.9519 - val_loss: 0.2115 - val_accuracy: 0.9357 - 73s/epoch - 34ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 73s - loss: 0.1258 - accuracy: 0.9538 - val_loss: 0.1744 - val_accuracy: 0.9427 - 73s/epoch - 34ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 73s - loss: 0.1173 - accuracy: 0.9569 - val_loss: 0.1340 - val_accuracy: 0.9535 - 73s/epoch - 34ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 73s - loss: 0.1059 - accuracy: 0.9616 - val_loss: 0.1253 - val_accuracy: 0.9598 - 73s/epoch - 34ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 73s - loss: 0.1042 - accuracy: 0.9620 - val_loss: 0.4096 - val_accuracy: 0.8478 - 73s/epoch - 34ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 73s - loss: 0.0935 - accuracy: 0.9647 - val_loss: 0.1603 - val_accuracy: 0.9447 - 73s/epoch - 35ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 73s - loss: 0.0963 - accuracy: 0.9649 - val_loss: 0.1075 - val_accuracy: 0.9640 - 73s/epoch - 34ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 73s - loss: 0.0878 - accuracy: 0.9688 - val_loss: 0.1107 - val_accuracy: 0.9660 - 73s/epoch - 34ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 72s - loss: 0.0837 - accuracy: 0.9711 - val_loss: 0.1085 - val_accuracy: 0.9640 - 72s/epoch - 34ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 73s - loss: 0.0863 - accuracy: 0.9701 - val_loss: 0.1077 - val_accuracy: 0.9660 - 73s/epoch - 34ms/step\n",
      "Epoch 16/30\n",
      "2116/2116 - 74s - loss: 0.0754 - accuracy: 0.9729 - val_loss: 0.1188 - val_accuracy: 0.9600 - 74s/epoch - 35ms/step\n",
      "Epoch 17/30\n",
      "2116/2116 - 74s - loss: 0.0694 - accuracy: 0.9753 - val_loss: 0.1702 - val_accuracy: 0.9456 - 74s/epoch - 35ms/step\n",
      "Epoch 17: early stopping\n",
      "Performance ofInceptionResNetV2: \n",
      "529/529 - 13s - loss: 0.1897 - accuracy: 0.9461 - 13s/epoch - 24ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: InceptionV3\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_7   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,813,029\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 41s - loss: 0.3535 - accuracy: 0.8664 - val_loss: 0.2246 - val_accuracy: 0.9235 - 41s/epoch - 19ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 38s - loss: 0.2018 - accuracy: 0.9259 - val_loss: 0.1828 - val_accuracy: 0.9320 - 38s/epoch - 18ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 38s - loss: 0.1694 - accuracy: 0.9376 - val_loss: 0.2007 - val_accuracy: 0.9328 - 38s/epoch - 18ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 38s - loss: 0.1340 - accuracy: 0.9493 - val_loss: 0.1790 - val_accuracy: 0.9325 - 38s/epoch - 18ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 38s - loss: 0.1319 - accuracy: 0.9492 - val_loss: 0.2414 - val_accuracy: 0.9161 - 38s/epoch - 18ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 38s - loss: 0.1148 - accuracy: 0.9572 - val_loss: 0.1715 - val_accuracy: 0.9444 - 38s/epoch - 18ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 38s - loss: 0.0999 - accuracy: 0.9629 - val_loss: 0.1548 - val_accuracy: 0.9425 - 38s/epoch - 18ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 38s - loss: 0.1019 - accuracy: 0.9624 - val_loss: 0.1751 - val_accuracy: 0.9419 - 38s/epoch - 18ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 38s - loss: 0.0853 - accuracy: 0.9670 - val_loss: 0.2081 - val_accuracy: 0.9297 - 38s/epoch - 18ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 38s - loss: 0.0878 - accuracy: 0.9683 - val_loss: 0.1650 - val_accuracy: 0.9442 - 38s/epoch - 18ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 38s - loss: 0.0774 - accuracy: 0.9706 - val_loss: 0.1598 - val_accuracy: 0.9510 - 38s/epoch - 18ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 38s - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.1716 - val_accuracy: 0.9481 - 38s/epoch - 18ms/step\n",
      "Epoch 12: early stopping\n",
      "Performance ofInceptionV3: \n",
      "529/529 - 7s - loss: 0.1602 - accuracy: 0.9504 - 7s/epoch - 12ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: MobileNet\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_224 (Functio  (None, None, None, 1024)  3228864  \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_8   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,233,989\n",
      "Trainable params: 5,125\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 15s - loss: 0.2892 - accuracy: 0.8990 - val_loss: 0.1359 - val_accuracy: 0.9546 - 15s/epoch - 7ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 14s - loss: 0.1305 - accuracy: 0.9563 - val_loss: 0.1425 - val_accuracy: 0.9422 - 14s/epoch - 6ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 14s - loss: 0.0952 - accuracy: 0.9657 - val_loss: 0.1387 - val_accuracy: 0.9478 - 14s/epoch - 6ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 14s - loss: 0.0761 - accuracy: 0.9733 - val_loss: 0.1390 - val_accuracy: 0.9512 - 14s/epoch - 6ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 14s - loss: 0.0643 - accuracy: 0.9778 - val_loss: 0.0869 - val_accuracy: 0.9694 - 14s/epoch - 6ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 14s - loss: 0.0595 - accuracy: 0.9796 - val_loss: 0.0878 - val_accuracy: 0.9685 - 14s/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 14s - loss: 0.0536 - accuracy: 0.9816 - val_loss: 0.0881 - val_accuracy: 0.9697 - 14s/epoch - 7ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 14s - loss: 0.0479 - accuracy: 0.9833 - val_loss: 0.0864 - val_accuracy: 0.9705 - 14s/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 13s - loss: 0.0441 - accuracy: 0.9836 - val_loss: 0.1052 - val_accuracy: 0.9674 - 13s/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 14s - loss: 0.0411 - accuracy: 0.9850 - val_loss: 0.1062 - val_accuracy: 0.9603 - 14s/epoch - 6ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 13s - loss: 0.0342 - accuracy: 0.9874 - val_loss: 0.0931 - val_accuracy: 0.9694 - 13s/epoch - 6ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 13s - loss: 0.0343 - accuracy: 0.9884 - val_loss: 0.0824 - val_accuracy: 0.9728 - 13s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 14s - loss: 0.0333 - accuracy: 0.9881 - val_loss: 0.0902 - val_accuracy: 0.9697 - 14s/epoch - 7ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "2116/2116 - 14s - loss: 0.0313 - accuracy: 0.9895 - val_loss: 0.1203 - val_accuracy: 0.9589 - 14s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 14s - loss: 0.0291 - accuracy: 0.9887 - val_loss: 0.0890 - val_accuracy: 0.9688 - 14s/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "2116/2116 - 14s - loss: 0.0271 - accuracy: 0.9902 - val_loss: 0.0992 - val_accuracy: 0.9691 - 14s/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "2116/2116 - 13s - loss: 0.0282 - accuracy: 0.9904 - val_loss: 0.1049 - val_accuracy: 0.9668 - 13s/epoch - 6ms/step\n",
      "Epoch 17: early stopping\n",
      "Performance ofMobileNet: \n",
      "529/529 - 2s - loss: 0.1162 - accuracy: 0.9669 - 2s/epoch - 5ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: MobileNetV2\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, None, None, 1280)  2257984  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_9   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 5)                 6405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 18s - loss: 0.3344 - accuracy: 0.8781 - val_loss: 0.2181 - val_accuracy: 0.9195 - 18s/epoch - 9ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 16s - loss: 0.1655 - accuracy: 0.9410 - val_loss: 0.1554 - val_accuracy: 0.9490 - 16s/epoch - 8ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 16s - loss: 0.1269 - accuracy: 0.9547 - val_loss: 0.1654 - val_accuracy: 0.9399 - 16s/epoch - 7ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 16s - loss: 0.1040 - accuracy: 0.9631 - val_loss: 0.1362 - val_accuracy: 0.9487 - 16s/epoch - 8ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 16s - loss: 0.0917 - accuracy: 0.9698 - val_loss: 0.1290 - val_accuracy: 0.9549 - 16s/epoch - 7ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 16s - loss: 0.0777 - accuracy: 0.9736 - val_loss: 0.1285 - val_accuracy: 0.9541 - 16s/epoch - 8ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 16s - loss: 0.0678 - accuracy: 0.9747 - val_loss: 0.1638 - val_accuracy: 0.9365 - 16s/epoch - 8ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 16s - loss: 0.0642 - accuracy: 0.9768 - val_loss: 0.1264 - val_accuracy: 0.9580 - 16s/epoch - 8ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 16s - loss: 0.0580 - accuracy: 0.9785 - val_loss: 0.1255 - val_accuracy: 0.9555 - 16s/epoch - 8ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 16s - loss: 0.0518 - accuracy: 0.9818 - val_loss: 0.1253 - val_accuracy: 0.9583 - 16s/epoch - 8ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 16s - loss: 0.0508 - accuracy: 0.9823 - val_loss: 0.1316 - val_accuracy: 0.9555 - 16s/epoch - 8ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 16s - loss: 0.0445 - accuracy: 0.9851 - val_loss: 0.1436 - val_accuracy: 0.9510 - 16s/epoch - 8ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 16s - loss: 0.0431 - accuracy: 0.9862 - val_loss: 0.1444 - val_accuracy: 0.9572 - 16s/epoch - 8ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 16s - loss: 0.0410 - accuracy: 0.9853 - val_loss: 0.1401 - val_accuracy: 0.9563 - 16s/epoch - 7ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 16s - loss: 0.0405 - accuracy: 0.9856 - val_loss: 0.1450 - val_accuracy: 0.9546 - 16s/epoch - 8ms/step\n",
      "Epoch 15: early stopping\n",
      "Performance ofMobileNetV2: \n",
      "529/529 - 3s - loss: 0.1444 - accuracy: 0.9556 - 3s/epoch - 6ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: Xception\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, None, None, 2048)  20861480  \n",
      "                                                                 \n",
      " global_average_pooling2d_10  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,871,725\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 35s - loss: 0.3189 - accuracy: 0.8901 - val_loss: 0.1885 - val_accuracy: 0.9376 - 35s/epoch - 17ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 33s - loss: 0.1645 - accuracy: 0.9453 - val_loss: 0.1601 - val_accuracy: 0.9487 - 33s/epoch - 16ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 34s - loss: 0.1242 - accuracy: 0.9594 - val_loss: 0.1314 - val_accuracy: 0.9549 - 34s/epoch - 16ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 33s - loss: 0.1014 - accuracy: 0.9662 - val_loss: 0.1162 - val_accuracy: 0.9606 - 33s/epoch - 16ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 34s - loss: 0.0853 - accuracy: 0.9718 - val_loss: 0.1143 - val_accuracy: 0.9563 - 34s/epoch - 16ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 33s - loss: 0.0762 - accuracy: 0.9745 - val_loss: 0.1138 - val_accuracy: 0.9609 - 33s/epoch - 16ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 34s - loss: 0.0698 - accuracy: 0.9757 - val_loss: 0.1107 - val_accuracy: 0.9620 - 34s/epoch - 16ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 33s - loss: 0.0618 - accuracy: 0.9784 - val_loss: 0.1137 - val_accuracy: 0.9586 - 33s/epoch - 16ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 34s - loss: 0.0576 - accuracy: 0.9817 - val_loss: 0.1101 - val_accuracy: 0.9612 - 34s/epoch - 16ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 34s - loss: 0.0503 - accuracy: 0.9801 - val_loss: 0.1449 - val_accuracy: 0.9518 - 34s/epoch - 16ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 33s - loss: 0.0461 - accuracy: 0.9839 - val_loss: 0.1080 - val_accuracy: 0.9617 - 33s/epoch - 16ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 34s - loss: 0.0427 - accuracy: 0.9878 - val_loss: 0.1100 - val_accuracy: 0.9615 - 34s/epoch - 16ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 33s - loss: 0.0406 - accuracy: 0.9863 - val_loss: 0.1128 - val_accuracy: 0.9632 - 33s/epoch - 16ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 35s - loss: 0.0377 - accuracy: 0.9875 - val_loss: 0.1608 - val_accuracy: 0.9501 - 35s/epoch - 16ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 34s - loss: 0.0355 - accuracy: 0.9883 - val_loss: 0.1179 - val_accuracy: 0.9558 - 34s/epoch - 16ms/step\n",
      "Epoch 16/30\n",
      "2116/2116 - 35s - loss: 0.0331 - accuracy: 0.9894 - val_loss: 0.1166 - val_accuracy: 0.9640 - 35s/epoch - 16ms/step\n",
      "Epoch 16: early stopping\n",
      "Performance ofXception: \n",
      "529/529 - 6s - loss: 0.1130 - accuracy: 0.9693 - 6s/epoch - 12ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: DenseNet121\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet121 (Functional)    (None, None, None, 1024)  7037504   \n",
      "                                                                 \n",
      " global_average_pooling2d_11  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,042,629\n",
      "Trainable params: 5,125\n",
      "Non-trainable params: 7,037,504\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 60s - loss: 0.3835 - accuracy: 0.8583 - val_loss: 0.2016 - val_accuracy: 0.9272 - 60s/epoch - 28ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 54s - loss: 0.1843 - accuracy: 0.9326 - val_loss: 0.1556 - val_accuracy: 0.9487 - 54s/epoch - 26ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "2116/2116 - 55s - loss: 0.1470 - accuracy: 0.9464 - val_loss: 0.1309 - val_accuracy: 0.9535 - 55s/epoch - 26ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 53s - loss: 0.1269 - accuracy: 0.9568 - val_loss: 0.1483 - val_accuracy: 0.9453 - 53s/epoch - 25ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 53s - loss: 0.1156 - accuracy: 0.9592 - val_loss: 0.1090 - val_accuracy: 0.9646 - 53s/epoch - 25ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 53s - loss: 0.1057 - accuracy: 0.9618 - val_loss: 0.1208 - val_accuracy: 0.9575 - 53s/epoch - 25ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 53s - loss: 0.0990 - accuracy: 0.9643 - val_loss: 0.1155 - val_accuracy: 0.9598 - 53s/epoch - 25ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 53s - loss: 0.0913 - accuracy: 0.9663 - val_loss: 0.1047 - val_accuracy: 0.9606 - 53s/epoch - 25ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 53s - loss: 0.0867 - accuracy: 0.9693 - val_loss: 0.1103 - val_accuracy: 0.9609 - 53s/epoch - 25ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 53s - loss: 0.0812 - accuracy: 0.9721 - val_loss: 0.1144 - val_accuracy: 0.9589 - 53s/epoch - 25ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 53s - loss: 0.0794 - accuracy: 0.9715 - val_loss: 0.1463 - val_accuracy: 0.9484 - 53s/epoch - 25ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 53s - loss: 0.0782 - accuracy: 0.9731 - val_loss: 0.1012 - val_accuracy: 0.9626 - 53s/epoch - 25ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 53s - loss: 0.0740 - accuracy: 0.9722 - val_loss: 0.1184 - val_accuracy: 0.9569 - 53s/epoch - 25ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 54s - loss: 0.0733 - accuracy: 0.9735 - val_loss: 0.1042 - val_accuracy: 0.9626 - 54s/epoch - 25ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 53s - loss: 0.0702 - accuracy: 0.9726 - val_loss: 0.1062 - val_accuracy: 0.9632 - 53s/epoch - 25ms/step\n",
      "Epoch 16/30\n",
      "2116/2116 - 53s - loss: 0.0692 - accuracy: 0.9747 - val_loss: 0.1274 - val_accuracy: 0.9558 - 53s/epoch - 25ms/step\n",
      "Epoch 17/30\n",
      "2116/2116 - 53s - loss: 0.0699 - accuracy: 0.9741 - val_loss: 0.1029 - val_accuracy: 0.9651 - 53s/epoch - 25ms/step\n",
      "Epoch 17: early stopping\n",
      "Performance ofDenseNet121: \n",
      "529/529 - 9s - loss: 0.1100 - accuracy: 0.9603 - 9s/epoch - 18ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: DenseNet169\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet169 (Functional)    (None, None, None, 1664)  12642880  \n",
      "                                                                 \n",
      " global_average_pooling2d_12  (None, 1664)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 5)                 8325      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,651,205\n",
      "Trainable params: 8,325\n",
      "Non-trainable params: 12,642,880\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 75s - loss: 0.2983 - accuracy: 0.8885 - val_loss: 0.1462 - val_accuracy: 0.9510 - 75s/epoch - 35ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 69s - loss: 0.1425 - accuracy: 0.9499 - val_loss: 0.1129 - val_accuracy: 0.9643 - 69s/epoch - 33ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 70s - loss: 0.1159 - accuracy: 0.9583 - val_loss: 0.1300 - val_accuracy: 0.9538 - 70s/epoch - 33ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 69s - loss: 0.0971 - accuracy: 0.9648 - val_loss: 0.0990 - val_accuracy: 0.9683 - 69s/epoch - 33ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 69s - loss: 0.0893 - accuracy: 0.9689 - val_loss: 0.1016 - val_accuracy: 0.9640 - 69s/epoch - 33ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 69s - loss: 0.0798 - accuracy: 0.9722 - val_loss: 0.0945 - val_accuracy: 0.9680 - 69s/epoch - 33ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 69s - loss: 0.0724 - accuracy: 0.9751 - val_loss: 0.1361 - val_accuracy: 0.9552 - 69s/epoch - 33ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 69s - loss: 0.0696 - accuracy: 0.9738 - val_loss: 0.1212 - val_accuracy: 0.9566 - 69s/epoch - 32ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 69s - loss: 0.0659 - accuracy: 0.9768 - val_loss: 0.0933 - val_accuracy: 0.9683 - 69s/epoch - 33ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 69s - loss: 0.0614 - accuracy: 0.9779 - val_loss: 0.1007 - val_accuracy: 0.9683 - 69s/epoch - 33ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 69s - loss: 0.0557 - accuracy: 0.9804 - val_loss: 0.0976 - val_accuracy: 0.9683 - 69s/epoch - 33ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 69s - loss: 0.0538 - accuracy: 0.9825 - val_loss: 0.0935 - val_accuracy: 0.9705 - 69s/epoch - 33ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 69s - loss: 0.0533 - accuracy: 0.9798 - val_loss: 0.1014 - val_accuracy: 0.9674 - 69s/epoch - 33ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 69s - loss: 0.0516 - accuracy: 0.9798 - val_loss: 0.0938 - val_accuracy: 0.9700 - 69s/epoch - 33ms/step\n",
      "Epoch 14: early stopping\n",
      "Performance ofDenseNet169: \n",
      "529/529 - 12s - loss: 0.0976 - accuracy: 0.9730 - 12s/epoch - 23ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: DenseNet201\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet201 (Functional)    (None, None, None, 1920)  18321984  \n",
      "                                                                 \n",
      " global_average_pooling2d_13  (None, 1920)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 5)                 9605      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,331,589\n",
      "Trainable params: 9,605\n",
      "Non-trainable params: 18,321,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 91s - loss: 0.2928 - accuracy: 0.8933 - val_loss: 0.1665 - val_accuracy: 0.9459 - 91s/epoch - 43ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 84s - loss: 0.1441 - accuracy: 0.9497 - val_loss: 0.1226 - val_accuracy: 0.9592 - 84s/epoch - 40ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 84s - loss: 0.1117 - accuracy: 0.9611 - val_loss: 0.1216 - val_accuracy: 0.9569 - 84s/epoch - 40ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 84s - loss: 0.0900 - accuracy: 0.9679 - val_loss: 0.1022 - val_accuracy: 0.9671 - 84s/epoch - 39ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 84s - loss: 0.0798 - accuracy: 0.9726 - val_loss: 0.1168 - val_accuracy: 0.9640 - 84s/epoch - 40ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 84s - loss: 0.0713 - accuracy: 0.9753 - val_loss: 0.2393 - val_accuracy: 0.9053 - 84s/epoch - 40ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 83s - loss: 0.0626 - accuracy: 0.9787 - val_loss: 0.1159 - val_accuracy: 0.9609 - 83s/epoch - 39ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 84s - loss: 0.0643 - accuracy: 0.9764 - val_loss: 0.1049 - val_accuracy: 0.9663 - 84s/epoch - 40ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 83s - loss: 0.0589 - accuracy: 0.9758 - val_loss: 0.1005 - val_accuracy: 0.9663 - 83s/epoch - 39ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 84s - loss: 0.0502 - accuracy: 0.9819 - val_loss: 0.0992 - val_accuracy: 0.9671 - 84s/epoch - 39ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 83s - loss: 0.0517 - accuracy: 0.9813 - val_loss: 0.1087 - val_accuracy: 0.9680 - 83s/epoch - 39ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 83s - loss: 0.0459 - accuracy: 0.9845 - val_loss: 0.1146 - val_accuracy: 0.9671 - 83s/epoch - 39ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 83s - loss: 0.0442 - accuracy: 0.9832 - val_loss: 0.1355 - val_accuracy: 0.9615 - 83s/epoch - 39ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 83s - loss: 0.0422 - accuracy: 0.9844 - val_loss: 0.0968 - val_accuracy: 0.9700 - 83s/epoch - 39ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 84s - loss: 0.0422 - accuracy: 0.9844 - val_loss: 0.1123 - val_accuracy: 0.9651 - 84s/epoch - 39ms/step\n",
      "Epoch 16/30\n",
      "2116/2116 - 84s - loss: 0.0397 - accuracy: 0.9843 - val_loss: 0.1038 - val_accuracy: 0.9694 - 84s/epoch - 39ms/step\n",
      "Epoch 17/30\n",
      "2116/2116 - 83s - loss: 0.0358 - accuracy: 0.9864 - val_loss: 0.1298 - val_accuracy: 0.9569 - 83s/epoch - 39ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "2116/2116 - 83s - loss: 0.0355 - accuracy: 0.9872 - val_loss: 0.1150 - val_accuracy: 0.9677 - 83s/epoch - 39ms/step\n",
      "Epoch 19/30\n",
      "2116/2116 - 83s - loss: 0.0329 - accuracy: 0.9878 - val_loss: 0.1080 - val_accuracy: 0.9685 - 83s/epoch - 39ms/step\n",
      "Epoch 19: early stopping\n",
      "Performance ofDenseNet201: \n",
      "529/529 - 15s - loss: 0.1101 - accuracy: 0.9683 - 15s/epoch - 28ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: EfficientNetV2S\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetv2-s (Functiona  (None, None, None, 1280)  20331360 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d_14  (None, 1280)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 5)                 6405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,337,765\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 20,331,360\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 54s - loss: 0.3431 - accuracy: 0.8783 - val_loss: 0.1857 - val_accuracy: 0.9413 - 54s/epoch - 25ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 46s - loss: 0.1948 - accuracy: 0.9355 - val_loss: 0.1411 - val_accuracy: 0.9515 - 46s/epoch - 22ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 46s - loss: 0.1601 - accuracy: 0.9433 - val_loss: 0.1241 - val_accuracy: 0.9563 - 46s/epoch - 22ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 46s - loss: 0.1411 - accuracy: 0.9486 - val_loss: 0.1181 - val_accuracy: 0.9600 - 46s/epoch - 22ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 46s - loss: 0.1334 - accuracy: 0.9490 - val_loss: 0.1097 - val_accuracy: 0.9629 - 46s/epoch - 22ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 45s - loss: 0.1211 - accuracy: 0.9579 - val_loss: 0.0950 - val_accuracy: 0.9668 - 45s/epoch - 21ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 46s - loss: 0.1133 - accuracy: 0.9589 - val_loss: 0.0910 - val_accuracy: 0.9663 - 46s/epoch - 22ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 46s - loss: 0.1081 - accuracy: 0.9607 - val_loss: 0.0963 - val_accuracy: 0.9663 - 46s/epoch - 22ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 46s - loss: 0.1058 - accuracy: 0.9625 - val_loss: 0.0900 - val_accuracy: 0.9649 - 46s/epoch - 22ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 46s - loss: 0.1033 - accuracy: 0.9636 - val_loss: 0.0849 - val_accuracy: 0.9651 - 46s/epoch - 22ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 46s - loss: 0.1008 - accuracy: 0.9644 - val_loss: 0.0820 - val_accuracy: 0.9702 - 46s/epoch - 22ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 46s - loss: 0.0950 - accuracy: 0.9664 - val_loss: 0.0853 - val_accuracy: 0.9654 - 46s/epoch - 22ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 46s - loss: 0.0913 - accuracy: 0.9689 - val_loss: 0.0787 - val_accuracy: 0.9728 - 46s/epoch - 22ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 46s - loss: 0.0914 - accuracy: 0.9677 - val_loss: 0.0788 - val_accuracy: 0.9725 - 46s/epoch - 22ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 46s - loss: 0.0879 - accuracy: 0.9680 - val_loss: 0.0776 - val_accuracy: 0.9742 - 46s/epoch - 22ms/step\n",
      "Epoch 16/30\n",
      "2116/2116 - 46s - loss: 0.0865 - accuracy: 0.9688 - val_loss: 0.0734 - val_accuracy: 0.9734 - 46s/epoch - 22ms/step\n",
      "Epoch 17/30\n",
      "2116/2116 - 46s - loss: 0.0808 - accuracy: 0.9694 - val_loss: 0.0761 - val_accuracy: 0.9705 - 46s/epoch - 22ms/step\n",
      "Epoch 18/30\n",
      "2116/2116 - 46s - loss: 0.0821 - accuracy: 0.9694 - val_loss: 0.0769 - val_accuracy: 0.9719 - 46s/epoch - 22ms/step\n",
      "Epoch 19/30\n",
      "2116/2116 - 46s - loss: 0.0820 - accuracy: 0.9695 - val_loss: 0.0734 - val_accuracy: 0.9736 - 46s/epoch - 22ms/step\n",
      "Epoch 20/30\n",
      "2116/2116 - 46s - loss: 0.0789 - accuracy: 0.9719 - val_loss: 0.0716 - val_accuracy: 0.9719 - 46s/epoch - 22ms/step\n",
      "Epoch 21/30\n",
      "2116/2116 - 46s - loss: 0.0802 - accuracy: 0.9694 - val_loss: 0.0704 - val_accuracy: 0.9756 - 46s/epoch - 22ms/step\n",
      "Epoch 22/30\n",
      "2116/2116 - 46s - loss: 0.0756 - accuracy: 0.9716 - val_loss: 0.1059 - val_accuracy: 0.9580 - 46s/epoch - 22ms/step\n",
      "Epoch 23/30\n",
      "2116/2116 - 46s - loss: 0.0737 - accuracy: 0.9742 - val_loss: 0.0679 - val_accuracy: 0.9734 - 46s/epoch - 22ms/step\n",
      "Epoch 24/30\n",
      "2116/2116 - 46s - loss: 0.0796 - accuracy: 0.9709 - val_loss: 0.0672 - val_accuracy: 0.9736 - 46s/epoch - 22ms/step\n",
      "Epoch 25/30\n",
      "2116/2116 - 46s - loss: 0.0789 - accuracy: 0.9715 - val_loss: 0.0752 - val_accuracy: 0.9677 - 46s/epoch - 22ms/step\n",
      "Epoch 26/30\n",
      "2116/2116 - 46s - loss: 0.0748 - accuracy: 0.9702 - val_loss: 0.0683 - val_accuracy: 0.9756 - 46s/epoch - 22ms/step\n",
      "Epoch 27/30\n",
      "2116/2116 - 46s - loss: 0.0736 - accuracy: 0.9744 - val_loss: 0.0697 - val_accuracy: 0.9731 - 46s/epoch - 22ms/step\n",
      "Epoch 28/30\n",
      "2116/2116 - 46s - loss: 0.0771 - accuracy: 0.9719 - val_loss: 0.0613 - val_accuracy: 0.9799 - 46s/epoch - 22ms/step\n",
      "Epoch 29/30\n",
      "2116/2116 - 46s - loss: 0.0708 - accuracy: 0.9749 - val_loss: 0.0643 - val_accuracy: 0.9759 - 46s/epoch - 22ms/step\n",
      "Epoch 30/30\n",
      "2116/2116 - 46s - loss: 0.0692 - accuracy: 0.9748 - val_loss: 0.0684 - val_accuracy: 0.9736 - 46s/epoch - 22ms/step\n",
      "Performance ofEfficientNetV2S: \n",
      "529/529 - 8s - loss: 0.0965 - accuracy: 0.9688 - 8s/epoch - 15ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: EfficientNetV2M\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetv2-m (Functiona  (None, None, None, 1280)  53150388 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d_15  (None, 1280)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 5)                 6405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,156,793\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 53,150,388\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 88s - loss: 0.4220 - accuracy: 0.8562 - val_loss: 0.2650 - val_accuracy: 0.9050 - 88s/epoch - 41ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 76s - loss: 0.2538 - accuracy: 0.9145 - val_loss: 0.2108 - val_accuracy: 0.9291 - 76s/epoch - 36ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 76s - loss: 0.2168 - accuracy: 0.9252 - val_loss: 0.1802 - val_accuracy: 0.9416 - 76s/epoch - 36ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 76s - loss: 0.1905 - accuracy: 0.9352 - val_loss: 0.1564 - val_accuracy: 0.9495 - 76s/epoch - 36ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 75s - loss: 0.1744 - accuracy: 0.9407 - val_loss: 0.1513 - val_accuracy: 0.9464 - 75s/epoch - 36ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 76s - loss: 0.1618 - accuracy: 0.9458 - val_loss: 0.1354 - val_accuracy: 0.9586 - 76s/epoch - 36ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 76s - loss: 0.1517 - accuracy: 0.9472 - val_loss: 0.1355 - val_accuracy: 0.9541 - 76s/epoch - 36ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 75s - loss: 0.1398 - accuracy: 0.9534 - val_loss: 0.1307 - val_accuracy: 0.9552 - 75s/epoch - 36ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 76s - loss: 0.1363 - accuracy: 0.9546 - val_loss: 0.1270 - val_accuracy: 0.9561 - 76s/epoch - 36ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 76s - loss: 0.1344 - accuracy: 0.9539 - val_loss: 0.1281 - val_accuracy: 0.9580 - 76s/epoch - 36ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 75s - loss: 0.1306 - accuracy: 0.9549 - val_loss: 0.1335 - val_accuracy: 0.9538 - 75s/epoch - 36ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 76s - loss: 0.1275 - accuracy: 0.9560 - val_loss: 0.1249 - val_accuracy: 0.9592 - 76s/epoch - 36ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 76s - loss: 0.1248 - accuracy: 0.9578 - val_loss: 0.1106 - val_accuracy: 0.9620 - 76s/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "2116/2116 - 76s - loss: 0.1172 - accuracy: 0.9595 - val_loss: 0.1180 - val_accuracy: 0.9572 - 76s/epoch - 36ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 75s - loss: 0.1200 - accuracy: 0.9585 - val_loss: 0.1073 - val_accuracy: 0.9637 - 75s/epoch - 36ms/step\n",
      "Epoch 16/30\n",
      "2116/2116 - 76s - loss: 0.1130 - accuracy: 0.9603 - val_loss: 0.1059 - val_accuracy: 0.9663 - 76s/epoch - 36ms/step\n",
      "Epoch 17/30\n",
      "2116/2116 - 75s - loss: 0.1136 - accuracy: 0.9586 - val_loss: 0.1062 - val_accuracy: 0.9651 - 75s/epoch - 35ms/step\n",
      "Epoch 18/30\n",
      "2116/2116 - 75s - loss: 0.1144 - accuracy: 0.9582 - val_loss: 0.1032 - val_accuracy: 0.9640 - 75s/epoch - 35ms/step\n",
      "Epoch 19/30\n",
      "2116/2116 - 75s - loss: 0.1076 - accuracy: 0.9624 - val_loss: 0.1026 - val_accuracy: 0.9643 - 75s/epoch - 36ms/step\n",
      "Epoch 20/30\n",
      "2116/2116 - 76s - loss: 0.1051 - accuracy: 0.9644 - val_loss: 0.1008 - val_accuracy: 0.9660 - 76s/epoch - 36ms/step\n",
      "Epoch 21/30\n",
      "2116/2116 - 75s - loss: 0.1045 - accuracy: 0.9629 - val_loss: 0.1074 - val_accuracy: 0.9612 - 75s/epoch - 36ms/step\n",
      "Epoch 22/30\n",
      "2116/2116 - 75s - loss: 0.1069 - accuracy: 0.9618 - val_loss: 0.1005 - val_accuracy: 0.9640 - 75s/epoch - 36ms/step\n",
      "Epoch 23/30\n",
      "2116/2116 - 75s - loss: 0.1079 - accuracy: 0.9602 - val_loss: 0.0983 - val_accuracy: 0.9683 - 75s/epoch - 36ms/step\n",
      "Epoch 24/30\n",
      "2116/2116 - 76s - loss: 0.1012 - accuracy: 0.9628 - val_loss: 0.1015 - val_accuracy: 0.9649 - 76s/epoch - 36ms/step\n",
      "Epoch 25/30\n",
      "2116/2116 - 75s - loss: 0.1033 - accuracy: 0.9641 - val_loss: 0.1033 - val_accuracy: 0.9640 - 75s/epoch - 36ms/step\n",
      "Epoch 26/30\n",
      "2116/2116 - 75s - loss: 0.1021 - accuracy: 0.9617 - val_loss: 0.1013 - val_accuracy: 0.9643 - 75s/epoch - 35ms/step\n",
      "Epoch 27/30\n",
      "2116/2116 - 76s - loss: 0.0972 - accuracy: 0.9664 - val_loss: 0.0991 - val_accuracy: 0.9660 - 76s/epoch - 36ms/step\n",
      "Epoch 28/30\n",
      "2116/2116 - 75s - loss: 0.0955 - accuracy: 0.9662 - val_loss: 0.1085 - val_accuracy: 0.9617 - 75s/epoch - 36ms/step\n",
      "Epoch 28: early stopping\n",
      "Performance ofEfficientNetV2M: \n",
      "529/529 - 13s - loss: 0.1209 - accuracy: 0.9636 - 13s/epoch - 25ms/step\n",
      "Found 8463 images belonging to 5 classes.\n",
      "Found 3528 images belonging to 5 classes.\n",
      "Found 2115 images belonging to 5 classes.\n",
      "Start training of: EfficientNetV2L\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetv2-l (Functiona  (None, None, None, 1280)  117746848\n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d_16  (None, 1280)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 5)                 6405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,753,253\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 117,746,848\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2116/2116 - 144s - loss: 0.4368 - accuracy: 0.8451 - val_loss: 0.2787 - val_accuracy: 0.9028 - 144s/epoch - 68ms/step\n",
      "Epoch 2/30\n",
      "2116/2116 - 128s - loss: 0.2548 - accuracy: 0.9097 - val_loss: 0.2186 - val_accuracy: 0.9277 - 128s/epoch - 60ms/step\n",
      "Epoch 3/30\n",
      "2116/2116 - 128s - loss: 0.2106 - accuracy: 0.9284 - val_loss: 0.1735 - val_accuracy: 0.9450 - 128s/epoch - 60ms/step\n",
      "Epoch 4/30\n",
      "2116/2116 - 128s - loss: 0.1839 - accuracy: 0.9363 - val_loss: 0.1568 - val_accuracy: 0.9510 - 128s/epoch - 60ms/step\n",
      "Epoch 5/30\n",
      "2116/2116 - 127s - loss: 0.1711 - accuracy: 0.9376 - val_loss: 0.1491 - val_accuracy: 0.9484 - 127s/epoch - 60ms/step\n",
      "Epoch 6/30\n",
      "2116/2116 - 128s - loss: 0.1611 - accuracy: 0.9459 - val_loss: 0.1377 - val_accuracy: 0.9538 - 128s/epoch - 60ms/step\n",
      "Epoch 7/30\n",
      "2116/2116 - 127s - loss: 0.1491 - accuracy: 0.9498 - val_loss: 0.1315 - val_accuracy: 0.9569 - 127s/epoch - 60ms/step\n",
      "Epoch 8/30\n",
      "2116/2116 - 127s - loss: 0.1448 - accuracy: 0.9512 - val_loss: 0.1354 - val_accuracy: 0.9549 - 127s/epoch - 60ms/step\n",
      "Epoch 9/30\n",
      "2116/2116 - 128s - loss: 0.1395 - accuracy: 0.9516 - val_loss: 0.1230 - val_accuracy: 0.9580 - 128s/epoch - 60ms/step\n",
      "Epoch 10/30\n",
      "2116/2116 - 128s - loss: 0.1347 - accuracy: 0.9514 - val_loss: 0.1219 - val_accuracy: 0.9595 - 128s/epoch - 61ms/step\n",
      "Epoch 11/30\n",
      "2116/2116 - 128s - loss: 0.1314 - accuracy: 0.9552 - val_loss: 0.1206 - val_accuracy: 0.9612 - 128s/epoch - 61ms/step\n",
      "Epoch 12/30\n",
      "2116/2116 - 128s - loss: 0.1241 - accuracy: 0.9564 - val_loss: 0.1062 - val_accuracy: 0.9657 - 128s/epoch - 61ms/step\n",
      "Epoch 13/30\n",
      "2116/2116 - 128s - loss: 0.1189 - accuracy: 0.9579 - val_loss: 0.1044 - val_accuracy: 0.9674 - 128s/epoch - 61ms/step\n",
      "Epoch 14/30\n",
      "2116/2116 - 128s - loss: 0.1206 - accuracy: 0.9577 - val_loss: 0.1141 - val_accuracy: 0.9629 - 128s/epoch - 61ms/step\n",
      "Epoch 15/30\n",
      "2116/2116 - 128s - loss: 0.1155 - accuracy: 0.9596 - val_loss: 0.1009 - val_accuracy: 0.9668 - 128s/epoch - 61ms/step\n",
      "Epoch 16/30\n",
      "2116/2116 - 128s - loss: 0.1109 - accuracy: 0.9617 - val_loss: 0.1013 - val_accuracy: 0.9683 - 128s/epoch - 61ms/step\n",
      "Epoch 17/30\n",
      "2116/2116 - 128s - loss: 0.1076 - accuracy: 0.9621 - val_loss: 0.0994 - val_accuracy: 0.9674 - 128s/epoch - 61ms/step\n",
      "Epoch 18/30\n",
      "2116/2116 - 128s - loss: 0.1108 - accuracy: 0.9592 - val_loss: 0.1022 - val_accuracy: 0.9663 - 128s/epoch - 60ms/step\n",
      "Epoch 19/30\n",
      "2116/2116 - 128s - loss: 0.1074 - accuracy: 0.9618 - val_loss: 0.0995 - val_accuracy: 0.9677 - 128s/epoch - 61ms/step\n",
      "Epoch 20/30\n",
      "2116/2116 - 128s - loss: 0.1028 - accuracy: 0.9648 - val_loss: 0.0995 - val_accuracy: 0.9680 - 128s/epoch - 60ms/step\n",
      "Epoch 21/30\n",
      "2116/2116 - 128s - loss: 0.1025 - accuracy: 0.9654 - val_loss: 0.1042 - val_accuracy: 0.9632 - 128s/epoch - 60ms/step\n",
      "Epoch 22/30\n",
      "2116/2116 - 128s - loss: 0.1031 - accuracy: 0.9638 - val_loss: 0.0954 - val_accuracy: 0.9697 - 128s/epoch - 61ms/step\n",
      "Epoch 23/30\n",
      "2116/2116 - 128s - loss: 0.1042 - accuracy: 0.9623 - val_loss: 0.0942 - val_accuracy: 0.9683 - 128s/epoch - 61ms/step\n",
      "Epoch 24/30\n",
      "2116/2116 - 128s - loss: 0.1017 - accuracy: 0.9651 - val_loss: 0.0993 - val_accuracy: 0.9671 - 128s/epoch - 61ms/step\n",
      "Epoch 25/30\n",
      "2116/2116 - 128s - loss: 0.0990 - accuracy: 0.9636 - val_loss: 0.0985 - val_accuracy: 0.9668 - 128s/epoch - 60ms/step\n",
      "Epoch 26/30\n",
      "2116/2116 - 128s - loss: 0.0997 - accuracy: 0.9647 - val_loss: 0.0949 - val_accuracy: 0.9688 - 128s/epoch - 61ms/step\n",
      "Epoch 27/30\n",
      "2116/2116 - 128s - loss: 0.1033 - accuracy: 0.9630 - val_loss: 0.0900 - val_accuracy: 0.9722 - 128s/epoch - 61ms/step\n",
      "Epoch 28/30\n",
      "2116/2116 - 128s - loss: 0.1006 - accuracy: 0.9661 - val_loss: 0.0921 - val_accuracy: 0.9683 - 128s/epoch - 60ms/step\n",
      "Epoch 29/30\n",
      "2116/2116 - 128s - loss: 0.0971 - accuracy: 0.9669 - val_loss: 0.0898 - val_accuracy: 0.9705 - 128s/epoch - 61ms/step\n",
      "Epoch 30/30\n",
      "2116/2116 - 128s - loss: 0.0951 - accuracy: 0.9690 - val_loss: 0.0993 - val_accuracy: 0.9668 - 128s/epoch - 61ms/step\n",
      "Performance ofEfficientNetV2L: \n",
      "529/529 - 22s - loss: 0.1132 - accuracy: 0.9645 - 22s/epoch - 41ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate tensor image data batches with model specific preprocessing\n",
    "BASE_DIR = 'images/'\n",
    "names = [\"3003\", \"3004\", \"3021\", \"6091\", \"852929\"]\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "ppDict = {}\n",
    "ppDict[\"VGG16\"] = tf.keras.applications.vgg16.preprocess_input\n",
    "ppDict[\"VGG19\"] = tf.keras.applications.vgg19.preprocess_input\n",
    "ppDict[\"ResNet101V2\"] = tf.keras.applications.resnet_v2.preprocess_input\n",
    "ppDict[\"ResNet152V2\"] = tf.keras.applications.resnet_v2.preprocess_input\n",
    "ppDict[\"ResNet50\"] = tf.keras.applications.resnet50.preprocess_input\n",
    "ppDict[\"ResNet50V2\"] = tf.keras.applications.resnet_v2.preprocess_input\n",
    "ppDict[\"InceptionResNetV2\"] = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
    "ppDict[\"InceptionV3\"] = tf.keras.applications.inception_v3.preprocess_input\n",
    "ppDict[\"MobileNet\"] = tf.keras.applications.mobilenet.preprocess_input\n",
    "ppDict[\"MobileNetV2\"] = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "ppDict[\"Xception\"] = tf.keras.applications.xception.preprocess_input\n",
    "ppDict[\"DenseNet121\"] = tf.keras.applications.densenet.preprocess_input\n",
    "ppDict[\"DenseNet169\"] = tf.keras.applications.densenet.preprocess_input\n",
    "ppDict[\"DenseNet201\"] = tf.keras.applications.densenet.preprocess_input\n",
    "ppDict[\"EfficientNetV2S\"] = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "ppDict[\"EfficientNetV2M\"] = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "ppDict[\"EfficientNetV2L\"] = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "\n",
    "fitModels = {}\n",
    "\n",
    "for name, model in processed_models.items():\n",
    "    train_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name])\n",
    "    valid_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name])\n",
    "    test_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name])\n",
    "    \n",
    "    train_batches = train_gen.flow_from_directory(\n",
    "        BASE_DIR + 'train',\n",
    "        target_size=(224, 224),\n",
    "        class_mode='sparse',\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        color_mode=\"rgb\",\n",
    "        classes=names   \n",
    "    )\n",
    "\n",
    "    val_batches = valid_gen.flow_from_directory(\n",
    "        BASE_DIR + 'val',\n",
    "        target_size=(224, 224),\n",
    "        class_mode='sparse',\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        color_mode=\"rgb\",\n",
    "        classes=names\n",
    "    )\n",
    "\n",
    "    test_batches = test_gen.flow_from_directory(\n",
    "        BASE_DIR + 'test',\n",
    "        target_size=(224, 224),\n",
    "        class_mode='sparse',\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        color_mode=\"rgb\",\n",
    "        classes=names\n",
    "    )\n",
    "    \n",
    "    epochs = 30\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    print(\"Start training of: \" + name)\n",
    "    model.summary()\n",
    "    model.fit(train_batches, validation_data=val_batches,\n",
    "              callbacks=[early_stopping],\n",
    "              epochs=epochs, verbose=2)\n",
    "    print(\"Performance of\" + name + \": \")\n",
    "    model.evaluate(test_batches, verbose=2)\n",
    "    \n",
    "    \n",
    "    fitModels[name] = model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a53d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
