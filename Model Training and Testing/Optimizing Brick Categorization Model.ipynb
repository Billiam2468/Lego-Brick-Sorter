{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9aebe0",
   "metadata": {},
   "source": [
    "# This is the notebook for optimizing the model for categorizing our Lego bricks. Categories will be defined by Tom Alphin's Lego Brick Labels (v39)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5bc457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mlHelper' from '/home/billiam/Documents/Repos/Lego-Brick-Sorter/mlHelper.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import packages\n",
    "import os  \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import mlHelper as mlHelper\n",
    "\n",
    "importlib.reload(mlHelper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e93b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-2201.3362, shape=(), dtype=float32)\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/home/billiam/Documents/Repos/Lego-Brick-Sorter\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "HOME_DIR = \"/home/billiam/Documents/Repos/Lego-Brick-Sorter/\"\n",
    "os.chdir(HOME_DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c822fadc",
   "metadata": {},
   "source": [
    "# Comparing Transfer Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c645824f",
   "metadata": {},
   "source": [
    "We will be using transfer learning techniques on Keras pre-trained models to create our categorization model. We will first need to test out all of the different Keras Applications and discover which one will best work for our needs.\n",
    "\n",
    "\n",
    "\n",
    "**Training Dataset:**\n",
    "\n",
    "~~We will be using the 447 class training data set referenced in this paper:https://www.iccs-meeting.org/archive/iccs2022/papers/133520608.pdf~~\n",
    "\n",
    "We will be using our synthetic data set with 4 currently generated classes (1000 imgs each)\n",
    "\n",
    "\n",
    "**Testing:**\n",
    "\n",
    "~~We will be selecting 20 random classes of Lego brick to train our models on. Models will then be compared with each other to determine what will best work for our purposes~~\n",
    "\n",
    "These 4 classes have matching real world data that can be used as a true concrete test of accuracy\n",
    "\n",
    "**Data Augmentation:**\n",
    "\n",
    "Data will be augmented in order to create a more robust model:\n",
    "\n",
    "* Horizontal flip\n",
    "* Vertical flip\n",
    "* 360 range of rotation\n",
    "* 50.0 range of random channel shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663e295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of models we will test\n",
    "model_dict = {}\n",
    "model_dict[\"VGG16\"] = tf.keras.applications.VGG16\n",
    "model_dict[\"VGG19\"] = tf.keras.applications.VGG19\n",
    "model_dict[\"ResNet101V2\"] = tf.keras.applications.ResNet101V2\n",
    "# model_dict[\"ResNet152\"] = tf.keras.applications.ResNet152\n",
    "model_dict[\"ResNet152V2\"] = tf.keras.applications.ResNet152V2\n",
    "model_dict[\"ResNet50\"] = tf.keras.applications.ResNet50\n",
    "model_dict[\"ResNet50V2\"] = tf.keras.applications.ResNet50V2\n",
    "model_dict[\"InceptionResNetV2\"] = tf.keras.applications.InceptionResNetV2\n",
    "model_dict[\"InceptionV3\"] = tf.keras.applications.InceptionV3\n",
    "model_dict[\"MobileNet\"] = tf.keras.applications.MobileNet\n",
    "model_dict[\"MobileNetV2\"] = tf.keras.applications.MobileNetV2\n",
    "# model_dict[\"MobileNetV3\"] = tf.keras.applications.MobileNetV3 <- weird exceptions\n",
    "model_dict[\"Xception\"] = tf.keras.applications.Xception\n",
    "model_dict[\"DenseNet121\"] = tf.keras.applications.DenseNet121\n",
    "model_dict[\"DenseNet169\"] = tf.keras.applications.DenseNet169\n",
    "model_dict[\"DenseNet201\"] = tf.keras.applications.DenseNet201\n",
    "model_dict[\"EfficientNetV2S\"] = tf.keras.applications.EfficientNetV2S\n",
    "model_dict[\"EfficientNetV2M\"] = tf.keras.applications.EfficientNetV2M\n",
    "model_dict[\"EfficientNetV2L\"] = tf.keras.applications.EfficientNetV2L\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7063c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: VGG16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▋                                                                          | 1/17 [00:00<00:03,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: VGG19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▎                                                                     | 2/17 [00:00<00:03,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ResNet101V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█████████████▉                                                                 | 3/17 [00:02<00:12,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ResNet152V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████████████▌                                                            | 4/17 [00:04<00:19,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ResNet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|███████████████████████▏                                                       | 5/17 [00:05<00:15,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ResNet50V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███████████████████████████▉                                                   | 6/17 [00:06<00:12,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: InceptionResNetV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████████████████████▌                                              | 7/17 [00:09<00:17,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: InceptionV3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|█████████████████████████████████████▏                                         | 8/17 [00:10<00:14,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: MobileNet\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████▊                                     | 9/17 [00:10<00:09,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: MobileNetV2\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████████████████████████████████████████████▉                                | 10/17 [00:11<00:07,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Xception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████████████████████████████████████████████████▍                           | 11/17 [00:12<00:06,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: DenseNet121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████████████████████████████████████████████████████                       | 12/17 [00:14<00:06,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: DenseNet169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████████████████████████████▋                  | 13/17 [00:16<00:05,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: DenseNet201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████████████████████████████████████████████████████████████▏             | 14/17 [00:19<00:05,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: EfficientNetV2S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████▊         | 15/17 [00:21<00:04,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: EfficientNetV2M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████▍    | 16/17 [00:24<00:02,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: EfficientNetV2L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 17/17 [00:29<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load in our preprocessed models into a dictionary\n",
    "preprocessed_models = {}\n",
    "for model_name, model in tqdm(model_dict.items()):\n",
    "    print(\"Processing: \" + model_name)\n",
    "    preprocessed_models[model_name] = model(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9080c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: VGG16\n",
      "processing: VGG19\n",
      "processing: ResNet101V2\n",
      "processing: ResNet152V2\n",
      "processing: ResNet50\n",
      "processing: ResNet50V2\n",
      "processing: InceptionResNetV2\n",
      "processing: InceptionV3\n",
      "processing: MobileNet\n",
      "processing: MobileNetV2\n",
      "processing: Xception\n",
      "processing: DenseNet121\n",
      "processing: DenseNet169\n",
      "processing: DenseNet201\n",
      "processing: EfficientNetV2S\n",
      "processing: EfficientNetV2M\n",
      "processing: EfficientNetV2L\n"
     ]
    }
   ],
   "source": [
    "# Process our models\n",
    "# (delete the last layer, make all remaining layers untrainable, and add our own trainable layer)\n",
    "names = [\"3003\", \"3004\", \"3021\", \"6091\"]\n",
    "NUM_CLASSES = len(names)\n",
    "\n",
    "processed_models = {}\n",
    "for name, pre_model in preprocessed_models.items():\n",
    "    print(\"processing: \" + name)\n",
    "    # Create our empty model (look up sequential vs functional)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Add all layers from our pre-trained model (last layer already deleted from include_top=False)\n",
    "    model.add(pre_model)\n",
    "\n",
    "    # Make all remaining layers untrainable and add our last trainable layer\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Add our layers\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(1024, activation='relu')) # Dense layer for combining features model recognized in img\n",
    "    model.add(layers.Dropout(0.5)) # Dropout layer which may prevent overfitting and improve generalization ability (test-set)\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation='softmax')) # Final classification layer equal to number of classes\n",
    "    \n",
    "    # Loss and optimizer functions\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    optim = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Might try mean average precision for metric because we are categorizing so many classes\n",
    "    metrics = [\"categorical_accuracy\"]\n",
    "    \n",
    "    # Used to do accuracy but someone online recommended MAP\n",
    "    # https://www.reddit.com/r/learnmachinelearning/comments/xpyv8j/data_set_for_lego_image_classification_800000/\n",
    "    # metrics = [\"accuracy\"]\n",
    "\n",
    "    # Compile our model\n",
    "    model.compile(optimizer=optim, loss=loss, metrics=metrics)\n",
    "    \n",
    "    # Add model to our dict\n",
    "    processed_models[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952d7650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: VGG16\n",
      "Epoch 8: early stopping\n",
      "Performance ofVGG16: \n",
      "351/351 - 7s - loss: 1.0302 - categorical_accuracy: 0.7380 - 7s/epoch - 19ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: VGG16\n",
      "Epoch 7: early stopping\n",
      "Performance ofVGG16: \n",
      "351/351 - 6s - loss: 1.2795 - categorical_accuracy: 0.7509 - 6s/epoch - 17ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: VGG16\n",
      "Epoch 9: early stopping\n",
      "Performance ofVGG16: \n",
      "351/351 - 6s - loss: 0.9152 - categorical_accuracy: 0.8166 - 6s/epoch - 17ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: VGG19\n",
      "Epoch 14: early stopping\n",
      "Performance ofVGG19: \n",
      "351/351 - 7s - loss: 1.3385 - categorical_accuracy: 0.7388 - 7s/epoch - 21ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: VGG19\n",
      "Epoch 10: early stopping\n",
      "Performance ofVGG19: \n",
      "351/351 - 7s - loss: 1.9755 - categorical_accuracy: 0.6581 - 7s/epoch - 21ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: VGG19\n",
      "Epoch 6: early stopping\n",
      "Performance ofVGG19: \n",
      "351/351 - 7s - loss: 1.5012 - categorical_accuracy: 0.7802 - 7s/epoch - 21ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet101V2\n",
      "Epoch 14: early stopping\n",
      "Performance ofResNet101V2: \n",
      "351/351 - 6s - loss: 0.8217 - categorical_accuracy: 0.8294 - 6s/epoch - 18ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet101V2\n",
      "Epoch 7: early stopping\n",
      "Performance ofResNet101V2: \n",
      "351/351 - 6s - loss: 1.1257 - categorical_accuracy: 0.6981 - 6s/epoch - 18ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet101V2\n",
      "Epoch 11: early stopping\n",
      "Performance ofResNet101V2: \n",
      "351/351 - 6s - loss: 1.0369 - categorical_accuracy: 0.7709 - 6s/epoch - 18ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet152V2\n",
      "Epoch 10: early stopping\n",
      "Performance ofResNet152V2: \n",
      "351/351 - 9s - loss: 1.0784 - categorical_accuracy: 0.7074 - 9s/epoch - 26ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet152V2\n",
      "Epoch 16: early stopping\n",
      "Performance ofResNet152V2: \n",
      "351/351 - 9s - loss: 1.8196 - categorical_accuracy: 0.6695 - 9s/epoch - 26ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet152V2\n",
      "Epoch 8: early stopping\n",
      "Performance ofResNet152V2: \n",
      "351/351 - 9s - loss: 1.6887 - categorical_accuracy: 0.7109 - 9s/epoch - 26ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet50\n",
      "Epoch 15: early stopping\n",
      "Performance ofResNet50: \n",
      "351/351 - 5s - loss: 0.6242 - categorical_accuracy: 0.7901 - 5s/epoch - 13ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet50\n",
      "Epoch 9: early stopping\n",
      "Performance ofResNet50: \n",
      "351/351 - 5s - loss: 0.9972 - categorical_accuracy: 0.7959 - 5s/epoch - 13ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet50\n",
      "Epoch 20: early stopping\n",
      "Performance ofResNet50: \n",
      "351/351 - 5s - loss: 0.6315 - categorical_accuracy: 0.8601 - 5s/epoch - 13ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet50V2\n",
      "Epoch 19: early stopping\n",
      "Performance ofResNet50V2: \n",
      "351/351 - 4s - loss: 0.6093 - categorical_accuracy: 0.8244 - 4s/epoch - 11ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet50V2\n",
      "Epoch 14: early stopping\n",
      "Performance ofResNet50V2: \n",
      "351/351 - 4s - loss: 1.3670 - categorical_accuracy: 0.6974 - 4s/epoch - 11ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: ResNet50V2\n",
      "Epoch 8: early stopping\n",
      "Performance ofResNet50V2: \n",
      "351/351 - 4s - loss: 0.7588 - categorical_accuracy: 0.8094 - 4s/epoch - 11ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: InceptionResNetV2\n",
      "Epoch 21: early stopping\n",
      "Performance ofInceptionResNetV2: \n",
      "351/351 - 9s - loss: 1.2307 - categorical_accuracy: 0.6495 - 9s/epoch - 24ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: InceptionResNetV2\n",
      "Epoch 7: early stopping\n",
      "Performance ofInceptionResNetV2: \n",
      "351/351 - 8s - loss: 0.8247 - categorical_accuracy: 0.7580 - 8s/epoch - 24ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: InceptionResNetV2\n",
      "Epoch 9: early stopping\n",
      "Performance ofInceptionResNetV2: \n",
      "351/351 - 8s - loss: 1.8445 - categorical_accuracy: 0.5125 - 8s/epoch - 24ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: InceptionV3\n",
      "Epoch 17: early stopping\n",
      "Performance ofInceptionV3: \n",
      "351/351 - 4s - loss: 0.8404 - categorical_accuracy: 0.6738 - 4s/epoch - 13ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: InceptionV3\n",
      "Epoch 7: early stopping\n",
      "Performance ofInceptionV3: \n",
      "351/351 - 4s - loss: 0.7932 - categorical_accuracy: 0.7166 - 4s/epoch - 13ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: InceptionV3\n",
      "Epoch 25: early stopping\n",
      "Performance ofInceptionV3: \n",
      "351/351 - 4s - loss: 0.6767 - categorical_accuracy: 0.7787 - 4s/epoch - 13ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: MobileNet\n",
      "Epoch 12: early stopping\n",
      "Performance ofMobileNet: \n",
      "351/351 - 2s - loss: 0.3524 - categorical_accuracy: 0.8958 - 2s/epoch - 4ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: MobileNet\n",
      "Epoch 6: early stopping\n",
      "Performance ofMobileNet: \n",
      "351/351 - 2s - loss: 1.3869 - categorical_accuracy: 0.5896 - 2s/epoch - 4ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: MobileNet\n",
      "Epoch 6: early stopping\n",
      "Performance ofMobileNet: \n",
      "351/351 - 2s - loss: 0.9636 - categorical_accuracy: 0.7566 - 2s/epoch - 4ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: MobileNetV2\n",
      "Epoch 17: early stopping\n",
      "Performance ofMobileNetV2: \n",
      "351/351 - 2s - loss: 1.1306 - categorical_accuracy: 0.5939 - 2s/epoch - 6ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training of: MobileNetV2\n",
      "Epoch 7: early stopping\n",
      "Performance ofMobileNetV2: \n",
      "351/351 - 2s - loss: 1.0092 - categorical_accuracy: 0.7016 - 2s/epoch - 5ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: MobileNetV2\n",
      "Epoch 16: early stopping\n",
      "Performance ofMobileNetV2: \n",
      "351/351 - 2s - loss: 1.6122 - categorical_accuracy: 0.6296 - 2s/epoch - 5ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: Xception\n",
      "Epoch 18: early stopping\n",
      "Performance ofXception: \n",
      "351/351 - 4s - loss: 0.8519 - categorical_accuracy: 0.7031 - 4s/epoch - 11ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: Xception\n",
      "Epoch 13: early stopping\n",
      "Performance ofXception: \n",
      "351/351 - 4s - loss: 0.9524 - categorical_accuracy: 0.6781 - 4s/epoch - 11ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: Xception\n",
      "Epoch 7: early stopping\n",
      "Performance ofXception: \n",
      "351/351 - 4s - loss: 0.9640 - categorical_accuracy: 0.7038 - 4s/epoch - 11ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet121\n",
      "Epoch 13: early stopping\n",
      "Performance ofDenseNet121: \n",
      "351/351 - 6s - loss: 0.8951 - categorical_accuracy: 0.6888 - 6s/epoch - 17ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet121\n",
      "Epoch 10: early stopping\n",
      "Performance ofDenseNet121: \n",
      "351/351 - 6s - loss: 1.2372 - categorical_accuracy: 0.6560 - 6s/epoch - 18ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet121\n",
      "Epoch 21: early stopping\n",
      "Performance ofDenseNet121: \n",
      "351/351 - 6s - loss: 0.8799 - categorical_accuracy: 0.7809 - 6s/epoch - 17ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet169\n",
      "Epoch 14: early stopping\n",
      "Performance ofDenseNet169: \n",
      "351/351 - 8s - loss: 2.1609 - categorical_accuracy: 0.3226 - 8s/epoch - 23ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet169\n",
      "Epoch 10: early stopping\n",
      "Performance ofDenseNet169: \n",
      "351/351 - 8s - loss: 1.3606 - categorical_accuracy: 0.6031 - 8s/epoch - 23ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet169\n",
      "Epoch 11: early stopping\n",
      "Performance ofDenseNet169: \n",
      "351/351 - 8s - loss: 1.4888 - categorical_accuracy: 0.7502 - 8s/epoch - 23ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet201\n",
      "Performance ofDenseNet201: \n",
      "351/351 - 10s - loss: 1.4571 - categorical_accuracy: 0.6296 - 10s/epoch - 28ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet201\n",
      "Epoch 14: early stopping\n",
      "Performance ofDenseNet201: \n",
      "351/351 - 10s - loss: 1.2995 - categorical_accuracy: 0.6453 - 10s/epoch - 28ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: DenseNet201\n",
      "Epoch 10: early stopping\n",
      "Performance ofDenseNet201: \n",
      "351/351 - 10s - loss: 1.7199 - categorical_accuracy: 0.5867 - 10s/epoch - 28ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2S\n",
      "Performance ofEfficientNetV2S: \n",
      "351/351 - 5s - loss: 0.9495 - categorical_accuracy: 0.7880 - 5s/epoch - 15ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2S\n",
      "Epoch 17: early stopping\n",
      "Performance ofEfficientNetV2S: \n",
      "351/351 - 5s - loss: 0.6373 - categorical_accuracy: 0.8601 - 5s/epoch - 15ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2S\n",
      "Epoch 9: early stopping\n",
      "Performance ofEfficientNetV2S: \n",
      "351/351 - 5s - loss: 1.0216 - categorical_accuracy: 0.7837 - 5s/epoch - 15ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2M\n",
      "Epoch 18: early stopping\n",
      "Performance ofEfficientNetV2M: \n",
      "351/351 - 9s - loss: 0.7675 - categorical_accuracy: 0.8208 - 9s/epoch - 25ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2M\n",
      "Epoch 8: early stopping\n",
      "Performance ofEfficientNetV2M: \n",
      "351/351 - 9s - loss: 1.4630 - categorical_accuracy: 0.6852 - 9s/epoch - 25ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2M\n",
      "Epoch 7: early stopping\n",
      "Performance ofEfficientNetV2M: \n",
      "351/351 - 9s - loss: 1.7056 - categorical_accuracy: 0.6795 - 9s/epoch - 24ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2L\n",
      "Epoch 19: early stopping\n",
      "Performance ofEfficientNetV2L: \n",
      "351/351 - 14s - loss: 0.9464 - categorical_accuracy: 0.7045 - 14s/epoch - 41ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2L\n",
      "Epoch 7: early stopping\n",
      "Performance ofEfficientNetV2L: \n",
      "351/351 - 14s - loss: 0.6790 - categorical_accuracy: 0.7944 - 14s/epoch - 41ms/step\n",
      "Found 3000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1401 images belonging to 4 classes.\n",
      "Start training of: EfficientNetV2L\n",
      "Epoch 8: early stopping\n",
      "Performance ofEfficientNetV2L: \n",
      "351/351 - 14s - loss: 0.8114 - categorical_accuracy: 0.7937 - 14s/epoch - 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>categorical_accuracy_it_1</th>\n",
       "      <th>categorical_accuracy_it_2</th>\n",
       "      <th>categorical_accuracy_it_3</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>0.790150</td>\n",
       "      <td>0.795860</td>\n",
       "      <td>0.860100</td>\n",
       "      <td>0.815370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EfficientNetV2S</td>\n",
       "      <td>0.788009</td>\n",
       "      <td>0.860100</td>\n",
       "      <td>0.783726</td>\n",
       "      <td>0.810611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ResNet50V2</td>\n",
       "      <td>0.824411</td>\n",
       "      <td>0.697359</td>\n",
       "      <td>0.809422</td>\n",
       "      <td>0.777064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VGG16</td>\n",
       "      <td>0.738044</td>\n",
       "      <td>0.750892</td>\n",
       "      <td>0.816560</td>\n",
       "      <td>0.768499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ResNet101V2</td>\n",
       "      <td>0.829408</td>\n",
       "      <td>0.698073</td>\n",
       "      <td>0.770878</td>\n",
       "      <td>0.766119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EfficientNetV2L</td>\n",
       "      <td>0.704497</td>\n",
       "      <td>0.794433</td>\n",
       "      <td>0.793719</td>\n",
       "      <td>0.764216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MobileNet</td>\n",
       "      <td>0.895789</td>\n",
       "      <td>0.589579</td>\n",
       "      <td>0.756602</td>\n",
       "      <td>0.747323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EfficientNetV2M</td>\n",
       "      <td>0.820842</td>\n",
       "      <td>0.685225</td>\n",
       "      <td>0.679515</td>\n",
       "      <td>0.728527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VGG19</td>\n",
       "      <td>0.738758</td>\n",
       "      <td>0.658101</td>\n",
       "      <td>0.780157</td>\n",
       "      <td>0.725672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>InceptionV3</td>\n",
       "      <td>0.673804</td>\n",
       "      <td>0.716631</td>\n",
       "      <td>0.778729</td>\n",
       "      <td>0.723055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DenseNet121</td>\n",
       "      <td>0.688794</td>\n",
       "      <td>0.655960</td>\n",
       "      <td>0.780871</td>\n",
       "      <td>0.708542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ResNet152V2</td>\n",
       "      <td>0.707352</td>\n",
       "      <td>0.669522</td>\n",
       "      <td>0.710921</td>\n",
       "      <td>0.695931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Xception</td>\n",
       "      <td>0.703069</td>\n",
       "      <td>0.678087</td>\n",
       "      <td>0.703783</td>\n",
       "      <td>0.694980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MobileNetV2</td>\n",
       "      <td>0.593862</td>\n",
       "      <td>0.701642</td>\n",
       "      <td>0.629550</td>\n",
       "      <td>0.641685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>InceptionResNetV2</td>\n",
       "      <td>0.649536</td>\n",
       "      <td>0.758030</td>\n",
       "      <td>0.512491</td>\n",
       "      <td>0.640019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>0.629550</td>\n",
       "      <td>0.645253</td>\n",
       "      <td>0.586724</td>\n",
       "      <td>0.620509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DenseNet169</td>\n",
       "      <td>0.322627</td>\n",
       "      <td>0.603141</td>\n",
       "      <td>0.750178</td>\n",
       "      <td>0.558649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  categorical_accuracy_it_1  categorical_accuracy_it_2  \\\n",
       "0            ResNet50                   0.790150                   0.795860   \n",
       "1     EfficientNetV2S                   0.788009                   0.860100   \n",
       "2          ResNet50V2                   0.824411                   0.697359   \n",
       "3               VGG16                   0.738044                   0.750892   \n",
       "4         ResNet101V2                   0.829408                   0.698073   \n",
       "5     EfficientNetV2L                   0.704497                   0.794433   \n",
       "6           MobileNet                   0.895789                   0.589579   \n",
       "7     EfficientNetV2M                   0.820842                   0.685225   \n",
       "8               VGG19                   0.738758                   0.658101   \n",
       "9         InceptionV3                   0.673804                   0.716631   \n",
       "10        DenseNet121                   0.688794                   0.655960   \n",
       "11        ResNet152V2                   0.707352                   0.669522   \n",
       "12           Xception                   0.703069                   0.678087   \n",
       "13        MobileNetV2                   0.593862                   0.701642   \n",
       "14  InceptionResNetV2                   0.649536                   0.758030   \n",
       "15        DenseNet201                   0.629550                   0.645253   \n",
       "16        DenseNet169                   0.322627                   0.603141   \n",
       "\n",
       "    categorical_accuracy_it_3       avg  \n",
       "0                    0.860100  0.815370  \n",
       "1                    0.783726  0.810611  \n",
       "2                    0.809422  0.777064  \n",
       "3                    0.816560  0.768499  \n",
       "4                    0.770878  0.766119  \n",
       "5                    0.793719  0.764216  \n",
       "6                    0.756602  0.747323  \n",
       "7                    0.679515  0.728527  \n",
       "8                    0.780157  0.725672  \n",
       "9                    0.778729  0.723055  \n",
       "10                   0.780871  0.708542  \n",
       "11                   0.710921  0.695931  \n",
       "12                   0.703783  0.694980  \n",
       "13                   0.629550  0.641685  \n",
       "14                   0.512491  0.640019  \n",
       "15                   0.586724  0.620509  \n",
       "16                   0.750178  0.558649  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate tensor image data batches with model specific preprocessing\n",
    "BASE_DIR = 'data/fakeData/'\n",
    "\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "ppDict = {}\n",
    "ppDict[\"VGG16\"] = tf.keras.applications.vgg16.preprocess_input\n",
    "ppDict[\"VGG19\"] = tf.keras.applications.vgg19.preprocess_input\n",
    "ppDict[\"ResNet101V2\"] = tf.keras.applications.resnet_v2.preprocess_input\n",
    "ppDict[\"ResNet152V2\"] = tf.keras.applications.resnet_v2.preprocess_input\n",
    "ppDict[\"ResNet50\"] = tf.keras.applications.resnet50.preprocess_input\n",
    "ppDict[\"ResNet50V2\"] = tf.keras.applications.resnet_v2.preprocess_input\n",
    "ppDict[\"InceptionResNetV2\"] = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
    "ppDict[\"InceptionV3\"] = tf.keras.applications.inception_v3.preprocess_input\n",
    "ppDict[\"MobileNet\"] = tf.keras.applications.mobilenet.preprocess_input\n",
    "ppDict[\"MobileNetV2\"] = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "ppDict[\"Xception\"] = tf.keras.applications.xception.preprocess_input\n",
    "ppDict[\"DenseNet121\"] = tf.keras.applications.densenet.preprocess_input\n",
    "ppDict[\"DenseNet169\"] = tf.keras.applications.densenet.preprocess_input\n",
    "ppDict[\"DenseNet201\"] = tf.keras.applications.densenet.preprocess_input\n",
    "ppDict[\"EfficientNetV2S\"] = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "ppDict[\"EfficientNetV2M\"] = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "ppDict[\"EfficientNetV2L\"] = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "\n",
    "fitModels = {}\n",
    "\n",
    "allMetrics = []\n",
    "\n",
    "numIteration = 3\n",
    "\n",
    "for name, model in processed_models.items():\n",
    "    \n",
    "    modelMetric = [name]\n",
    "    \n",
    "    for iteration in range(numIteration):\n",
    "        train_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name],\n",
    "                                                                horizontal_flip = True,\n",
    "                                                                vertical_flip = True,\n",
    "                                                                rotation_range = 360,\n",
    "                                                                channel_shift_range = 50.0,\n",
    "                                                                fill_mode = 'wrap')\n",
    "        valid_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name])\n",
    "        test_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name])\n",
    "\n",
    "        train_batches = train_gen.flow_from_directory(\n",
    "            BASE_DIR + 'train',\n",
    "            target_size=(224, 224),\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            color_mode=\"rgb\",\n",
    "            classes=names   \n",
    "        )\n",
    "\n",
    "        val_batches = valid_gen.flow_from_directory(\n",
    "            BASE_DIR + 'val',\n",
    "            target_size=(224, 224),\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            color_mode=\"rgb\",\n",
    "            classes=names\n",
    "        )\n",
    "\n",
    "        test_batches = test_gen.flow_from_directory(\n",
    "            BASE_DIR + 'test',\n",
    "            target_size=(224, 224),\n",
    "            batch_size=4,\n",
    "            shuffle=False,\n",
    "            color_mode=\"rgb\",\n",
    "            classes=names\n",
    "        )\n",
    "\n",
    "        epochs = 25\n",
    "\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        print(\"Start training of: \" + name)\n",
    "        #model.summary()\n",
    "        model.fit(train_batches, validation_data=val_batches,\n",
    "                  callbacks=[early_stopping],\n",
    "                  epochs=epochs, verbose=0)\n",
    "        print(\"Performance of\" + name + \": \")\n",
    "        results = model.evaluate(test_batches, verbose=2)\n",
    "    \n",
    "    \n",
    "        modelMetric.append(results[1])\n",
    "    \n",
    "    avg = np.mean(modelMetric[1:])\n",
    "    modelMetric.append(avg)\n",
    "    \n",
    "    allMetrics.append(modelMetric)\n",
    "\n",
    "    \n",
    "columns = [\"name\"]    \n",
    "for num in range(numIteration):\n",
    "    columns.append('categorical_accuracy_it_' + str(num+1))\n",
    "columns.append('avg')\n",
    "\n",
    "df_results = pd.DataFrame(allMetrics, \n",
    "                          columns = columns)\n",
    "df_results.sort_values(by='avg', ascending=False, inplace=True)\n",
    "df_results.reset_index(inplace=True,drop=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a53d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa572d0e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "**Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31d922e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>categorical_accuracy_it_1</th>\n",
       "      <th>categorical_accuracy_it_2</th>\n",
       "      <th>categorical_accuracy_it_3</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>0.790150</td>\n",
       "      <td>0.795860</td>\n",
       "      <td>0.860100</td>\n",
       "      <td>0.815370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EfficientNetV2S</td>\n",
       "      <td>0.788009</td>\n",
       "      <td>0.860100</td>\n",
       "      <td>0.783726</td>\n",
       "      <td>0.810611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ResNet50V2</td>\n",
       "      <td>0.824411</td>\n",
       "      <td>0.697359</td>\n",
       "      <td>0.809422</td>\n",
       "      <td>0.777064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VGG16</td>\n",
       "      <td>0.738044</td>\n",
       "      <td>0.750892</td>\n",
       "      <td>0.816560</td>\n",
       "      <td>0.768499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ResNet101V2</td>\n",
       "      <td>0.829408</td>\n",
       "      <td>0.698073</td>\n",
       "      <td>0.770878</td>\n",
       "      <td>0.766119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EfficientNetV2L</td>\n",
       "      <td>0.704497</td>\n",
       "      <td>0.794433</td>\n",
       "      <td>0.793719</td>\n",
       "      <td>0.764216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MobileNet</td>\n",
       "      <td>0.895789</td>\n",
       "      <td>0.589579</td>\n",
       "      <td>0.756602</td>\n",
       "      <td>0.747323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EfficientNetV2M</td>\n",
       "      <td>0.820842</td>\n",
       "      <td>0.685225</td>\n",
       "      <td>0.679515</td>\n",
       "      <td>0.728527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VGG19</td>\n",
       "      <td>0.738758</td>\n",
       "      <td>0.658101</td>\n",
       "      <td>0.780157</td>\n",
       "      <td>0.725672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>InceptionV3</td>\n",
       "      <td>0.673804</td>\n",
       "      <td>0.716631</td>\n",
       "      <td>0.778729</td>\n",
       "      <td>0.723055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DenseNet121</td>\n",
       "      <td>0.688794</td>\n",
       "      <td>0.655960</td>\n",
       "      <td>0.780871</td>\n",
       "      <td>0.708542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ResNet152V2</td>\n",
       "      <td>0.707352</td>\n",
       "      <td>0.669522</td>\n",
       "      <td>0.710921</td>\n",
       "      <td>0.695931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Xception</td>\n",
       "      <td>0.703069</td>\n",
       "      <td>0.678087</td>\n",
       "      <td>0.703783</td>\n",
       "      <td>0.694980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MobileNetV2</td>\n",
       "      <td>0.593862</td>\n",
       "      <td>0.701642</td>\n",
       "      <td>0.629550</td>\n",
       "      <td>0.641685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>InceptionResNetV2</td>\n",
       "      <td>0.649536</td>\n",
       "      <td>0.758030</td>\n",
       "      <td>0.512491</td>\n",
       "      <td>0.640019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>0.629550</td>\n",
       "      <td>0.645253</td>\n",
       "      <td>0.586724</td>\n",
       "      <td>0.620509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DenseNet169</td>\n",
       "      <td>0.322627</td>\n",
       "      <td>0.603141</td>\n",
       "      <td>0.750178</td>\n",
       "      <td>0.558649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  categorical_accuracy_it_1  categorical_accuracy_it_2  \\\n",
       "0            ResNet50                   0.790150                   0.795860   \n",
       "1     EfficientNetV2S                   0.788009                   0.860100   \n",
       "2          ResNet50V2                   0.824411                   0.697359   \n",
       "3               VGG16                   0.738044                   0.750892   \n",
       "4         ResNet101V2                   0.829408                   0.698073   \n",
       "5     EfficientNetV2L                   0.704497                   0.794433   \n",
       "6           MobileNet                   0.895789                   0.589579   \n",
       "7     EfficientNetV2M                   0.820842                   0.685225   \n",
       "8               VGG19                   0.738758                   0.658101   \n",
       "9         InceptionV3                   0.673804                   0.716631   \n",
       "10        DenseNet121                   0.688794                   0.655960   \n",
       "11        ResNet152V2                   0.707352                   0.669522   \n",
       "12           Xception                   0.703069                   0.678087   \n",
       "13        MobileNetV2                   0.593862                   0.701642   \n",
       "14  InceptionResNetV2                   0.649536                   0.758030   \n",
       "15        DenseNet201                   0.629550                   0.645253   \n",
       "16        DenseNet169                   0.322627                   0.603141   \n",
       "\n",
       "    categorical_accuracy_it_3       avg  \n",
       "0                    0.860100  0.815370  \n",
       "1                    0.783726  0.810611  \n",
       "2                    0.809422  0.777064  \n",
       "3                    0.816560  0.768499  \n",
       "4                    0.770878  0.766119  \n",
       "5                    0.793719  0.764216  \n",
       "6                    0.756602  0.747323  \n",
       "7                    0.679515  0.728527  \n",
       "8                    0.780157  0.725672  \n",
       "9                    0.778729  0.723055  \n",
       "10                   0.780871  0.708542  \n",
       "11                   0.710921  0.695931  \n",
       "12                   0.703783  0.694980  \n",
       "13                   0.629550  0.641685  \n",
       "14                   0.512491  0.640019  \n",
       "15                   0.586724  0.620509  \n",
       "16                   0.750178  0.558649  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preResults = df_results\n",
    "preResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ed0d8",
   "metadata": {},
   "source": [
    "From our model, we can see that our top 5 performers without fine-tuning are:\n",
    "\n",
    "1. ResNet50\n",
    "2. EfficientNetV2S\n",
    "3. ResNet50V2\n",
    "4. VGG16\n",
    "5. ResNet101V2\n",
    "\n",
    "From these, we can see lots of ResNet combined with EfficientNetV2S and VGG16 (expected top contender). Going forward, we will be comparing ResNet50 and EfficientNetV2S with fine-tuning testing to see which one will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e41af",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# Fine Tuning Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309067aa",
   "metadata": {},
   "source": [
    "Comparing ResNet50 and EfficientNetV2S by fine-tuning on our real-world data set.\n",
    "\n",
    "Since this data set is unbalanced, we will be using class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dedb775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 Fine-Tuning Layers: conv4 and conv5\n",
    "# EfficientNetV2S Fine-Tuning Layers: block5 and block 6\n",
    "\n",
    "preprocessed_models[\"ResNet50\"].trainable = True\n",
    "for layer in preprocessed_models[\"ResNet50\"].layers:\n",
    "    trainable = ('conv4' in layer.name or 'conv5' in layer.name)\n",
    "    layer.trainable = trainable\n",
    "\n",
    "preprocessed_models[\"EfficientNetV2S\"].trainable = True\n",
    "for layer in preprocessed_models[\"EfficientNetV2S\"].layers:\n",
    "    trainable = ('block5' in layer.name or 'block6' in layer.name)\n",
    "    layer.trainable = trainable\n",
    "    \n",
    "# Loss and optimizer functions\n",
    "loss_fine = keras.losses.CategoricalCrossentropy()\n",
    "optim_fine = keras.optimizers.Adam(learning_rate=0.00001) #Lower learning rate since fine tuning\n",
    "    \n",
    "#Metrics:\n",
    "metrics_fine = ['categorical_accuracy']\n",
    "\n",
    "processed_models[\"ResNet50\"].compile(optimizer=optim_fine, loss=loss_fine, metrics=metrics_fine)\n",
    "processed_models[\"EfficientNetV2S\"].compile(optimizer=optim_fine, loss=loss_fine, metrics=metrics_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "391a2fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 841 images belonging to 4 classes.\n",
      "Found 351 images belonging to 4 classes.\n",
      "Found 209 images belonging to 4 classes.\n",
      "Start fine-tuning of: ResNet50\n",
      "Epoch 23: early stopping\n",
      "Performance ofResNet50: \n",
      "53/53 - 1s - loss: 0.2177 - categorical_accuracy: 0.9378 - 769ms/epoch - 15ms/step\n",
      "Found 841 images belonging to 4 classes.\n",
      "Found 351 images belonging to 4 classes.\n",
      "Found 209 images belonging to 4 classes.\n",
      "Start fine-tuning of: ResNet50\n",
      "Epoch 8: early stopping\n",
      "Performance ofResNet50: \n",
      "53/53 - 1s - loss: 0.1615 - categorical_accuracy: 0.9569 - 780ms/epoch - 15ms/step\n",
      "Found 841 images belonging to 4 classes.\n",
      "Found 351 images belonging to 4 classes.\n",
      "Found 209 images belonging to 4 classes.\n",
      "Start fine-tuning of: ResNet50\n",
      "Epoch 6: early stopping\n",
      "Performance ofResNet50: \n",
      "53/53 - 1s - loss: 0.2415 - categorical_accuracy: 0.9569 - 758ms/epoch - 14ms/step\n",
      "Found 841 images belonging to 4 classes.\n",
      "Found 351 images belonging to 4 classes.\n",
      "Found 209 images belonging to 4 classes.\n",
      "Start fine-tuning of: EfficientNetV2S\n",
      "Epoch 17: early stopping\n",
      "Performance ofEfficientNetV2S: \n",
      "53/53 - 1s - loss: 0.2700 - categorical_accuracy: 0.8947 - 855ms/epoch - 16ms/step\n",
      "Found 841 images belonging to 4 classes.\n",
      "Found 351 images belonging to 4 classes.\n",
      "Found 209 images belonging to 4 classes.\n",
      "Start fine-tuning of: EfficientNetV2S\n",
      "Epoch 17: early stopping\n",
      "Performance ofEfficientNetV2S: \n",
      "53/53 - 1s - loss: 0.1419 - categorical_accuracy: 0.9617 - 840ms/epoch - 16ms/step\n",
      "Found 841 images belonging to 4 classes.\n",
      "Found 351 images belonging to 4 classes.\n",
      "Found 209 images belonging to 4 classes.\n",
      "Start fine-tuning of: EfficientNetV2S\n",
      "Epoch 8: early stopping\n",
      "Performance ofEfficientNetV2S: \n",
      "53/53 - 1s - loss: 0.1134 - categorical_accuracy: 0.9617 - 855ms/epoch - 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>categorical_accuracy_it_1</th>\n",
       "      <th>categorical_accuracy_it_2</th>\n",
       "      <th>categorical_accuracy_it_3</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>0.937799</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.950558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EfficientNetV2S</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.961722</td>\n",
       "      <td>0.961722</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name  categorical_accuracy_it_1  categorical_accuracy_it_2  \\\n",
       "0         ResNet50                   0.937799                   0.956938   \n",
       "1  EfficientNetV2S                   0.894737                   0.961722   \n",
       "\n",
       "   categorical_accuracy_it_3       avg  \n",
       "0                   0.956938  0.950558  \n",
       "1                   0.961722  0.939394  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate tensor image data batches with model specific preprocessing\n",
    "BASE_DIR = 'data/realData/'\n",
    "\n",
    "models = [\"ResNet50\", \"EfficientNetV2S\"]\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "ppDict = {}\n",
    "ppDict[\"ResNet50\"] = tf.keras.applications.resnet50.preprocess_input\n",
    "ppDict[\"EfficientNetV2S\"] = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "\n",
    "fitModels = {}\n",
    "\n",
    "allMetrics = []\n",
    "\n",
    "numIteration = 3\n",
    "\n",
    "for name, model in processed_models.items():\n",
    "    if name in models:\n",
    "\n",
    "        modelMetric = [name]\n",
    "\n",
    "        for iteration in range(numIteration):\n",
    "            train_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name],\n",
    "                                                                    horizontal_flip = True,\n",
    "                                                                    vertical_flip = True,\n",
    "                                                                    rotation_range = 360,\n",
    "                                                                    channel_shift_range = 50.0,\n",
    "                                                                    fill_mode = 'wrap')\n",
    "            valid_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name])\n",
    "            test_gen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=ppDict[name])\n",
    "\n",
    "            train_batches = train_gen.flow_from_directory(\n",
    "                BASE_DIR + 'train',\n",
    "                target_size=(224, 224),\n",
    "                batch_size=4,\n",
    "                shuffle=True,\n",
    "                color_mode=\"rgb\",\n",
    "                classes=names   \n",
    "            )\n",
    "\n",
    "            val_batches = valid_gen.flow_from_directory(\n",
    "                BASE_DIR + 'val',\n",
    "                target_size=(224, 224),\n",
    "                batch_size=4,\n",
    "                shuffle=True,\n",
    "                color_mode=\"rgb\",\n",
    "                classes=names\n",
    "            )\n",
    "\n",
    "            test_batches = test_gen.flow_from_directory(\n",
    "                BASE_DIR + 'test',\n",
    "                target_size=(224, 224),\n",
    "                batch_size=4,\n",
    "                shuffle=False,\n",
    "                color_mode=\"rgb\",\n",
    "                classes=names\n",
    "            )\n",
    "\n",
    "            epochs = 25\n",
    "\n",
    "            early_stopping = keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=5,\n",
    "                verbose=2\n",
    "            )\n",
    "\n",
    "            #Compute Weights:\n",
    "            class_weight = compute_class_weight(class_weight='balanced',\n",
    "                                        classes=np.unique(train_batches.classes),\n",
    "                                        y=train_batches.classes)\n",
    "            class_weight = {i : class_weight[i] for i in range(len(names))}\n",
    "\n",
    "\n",
    "            print(\"Start fine-tuning of: \" + name)\n",
    "\n",
    "            history_fine = model.fit(train_batches,\n",
    "                             validation_data=val_batches,\n",
    "                             callbacks=[early_stopping],\n",
    "                             epochs=epochs,\n",
    "                             verbose=0,\n",
    "                             shuffle=True,\n",
    "                            class_weight=class_weight)\n",
    "\n",
    "            print(\"Performance of\" + name + \": \")\n",
    "            results = model.evaluate(test_batches, verbose=2)\n",
    "\n",
    "\n",
    "            modelMetric.append(results[1])\n",
    "\n",
    "        avg = np.mean(modelMetric[1:])\n",
    "        modelMetric.append(avg)\n",
    "\n",
    "        allMetrics.append(modelMetric)\n",
    "\n",
    "    \n",
    "columns = [\"name\"]    \n",
    "for num in range(numIteration):\n",
    "    columns.append('categorical_accuracy_it_' + str(num+1))\n",
    "columns.append('avg')\n",
    "\n",
    "df_results = pd.DataFrame(allMetrics, \n",
    "                          columns = columns)\n",
    "df_results.sort_values(by='avg', ascending=False, inplace=True)\n",
    "df_results.reset_index(inplace=True,drop=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d6936ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>categorical_accuracy_it_1</th>\n",
       "      <th>categorical_accuracy_it_2</th>\n",
       "      <th>categorical_accuracy_it_3</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>0.937799</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>0.950558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EfficientNetV2S</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.961722</td>\n",
       "      <td>0.961722</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name  categorical_accuracy_it_1  categorical_accuracy_it_2  \\\n",
       "0         ResNet50                   0.937799                   0.956938   \n",
       "1  EfficientNetV2S                   0.894737                   0.961722   \n",
       "\n",
       "   categorical_accuracy_it_3       avg  \n",
       "0                   0.956938  0.950558  \n",
       "1                   0.961722  0.939394  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = df_results\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c1fa4",
   "metadata": {},
   "source": [
    "# Final Results: ResNet50 WILL BE OUR MODEL!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
