{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6ec3ea",
   "metadata": {},
   "source": [
    "This is the code that will be imported into our Raspberry Pi to process and send images to our computer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48242ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "# reload our external packages (tracker)\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from skimage import data, filters\n",
    "import tracker as track\n",
    "\n",
    "importlib.reload(track)\n",
    "\n",
    "# Directory to save training images\n",
    "\n",
    "# base_dir = \"/Users/williamlee/Documents/Git Repos/Lego-Brick-Sorter/Imaging Pipeline/testImages\"\n",
    "# base_dir = \"/home/billiam/Documents/Lego_Sorter/TestPipelineImages\"\n",
    "base_dir = \"/media/billiam/B176-98A3/Self Contained GUI/realBatches\"\n",
    "\n",
    "os.chdir(base_dir)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f778e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "save image\n",
      "2\n",
      "save image\n",
      "3\n",
      "save image\n",
      "4\n",
      "save image\n",
      "5\n",
      "save image\n",
      "6\n",
      "save image\n",
      "7\n",
      "save image\n",
      "8\n",
      "save image\n",
      "9\n",
      "save image\n",
      "10\n",
      "save image\n",
      "11\n",
      "save image\n",
      "12\n",
      "save image\n",
      "13\n",
      "save image\n",
      "14\n",
      "save image\n",
      "15\n",
      "save image\n",
      "16\n",
      "save image\n",
      "17\n",
      "save image\n",
      "18\n",
      "save image\n",
      "19\n",
      "save image\n",
      "20\n",
      "save image\n",
      "21\n",
      "save image\n",
      "22\n",
      "save image\n",
      "23\n",
      "save image\n",
      "24\n",
      "save image\n",
      "25\n",
      "save image\n",
      "26\n",
      "save image\n",
      "27\n",
      "save image\n",
      "28\n",
      "save image\n",
      "29\n",
      "save image\n",
      "30\n",
      "save image\n",
      "31\n",
      "save image\n",
      "32\n",
      "save image\n",
      "33\n",
      "save image\n",
      "34\n",
      "save image\n",
      "35\n",
      "save image\n",
      "36\n",
      "save image\n",
      "37\n",
      "save image\n",
      "38\n",
      "save image\n",
      "39\n",
      "save image\n",
      "40\n",
      "save image\n",
      "41\n",
      "save image\n",
      "42\n",
      "save image\n",
      "43\n",
      "save image\n",
      "44\n",
      "save image\n",
      "45\n",
      "save image\n",
      "46\n",
      "save image\n",
      "47\n",
      "save image\n",
      "48\n",
      "save image\n",
      "49\n",
      "save image\n",
      "50\n",
      "save image\n",
      "51\n",
      "save image\n",
      "52\n",
      "save image\n",
      "53\n",
      "save image\n",
      "54\n",
      "save image\n",
      "55\n",
      "save image\n",
      "56\n",
      "save image\n",
      "57\n",
      "save image\n",
      "58\n",
      "save image\n",
      "59\n",
      "save image\n",
      "60\n",
      "save image\n",
      "61\n",
      "save image\n",
      "62\n",
      "save image\n",
      "63\n",
      "save image\n",
      "64\n",
      "save image\n",
      "65\n",
      "save image\n",
      "66\n",
      "save image\n",
      "67\n",
      "save image\n",
      "68\n",
      "save image\n",
      "69\n",
      "save image\n",
      "70\n",
      "save image\n",
      "71\n",
      "save image\n",
      "72\n",
      "save image\n",
      "73\n",
      "save image\n",
      "74\n",
      "save image\n",
      "75\n",
      "save image\n",
      "76\n",
      "save image\n",
      "77\n",
      "save image\n",
      "78\n",
      "save image\n",
      "79\n",
      "save image\n",
      "80\n",
      "save image\n",
      "81\n",
      "save image\n",
      "82\n",
      "save image\n",
      "83\n",
      "save image\n",
      "84\n",
      "save image\n",
      "85\n",
      "save image\n",
      "86\n",
      "save image\n",
      "87\n",
      "save image\n",
      "88\n",
      "save image\n",
      "89\n",
      "save image\n"
     ]
    }
   ],
   "source": [
    "# pieceNum = \"3021\"\n",
    "# os.chdir(pieceNum)\n",
    "# Open Video\n",
    "#cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(cv2.CAP_V4L2)\n",
    "\n",
    "# Adjust Camera Settings\n",
    "cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1) # Manual exposure mode\n",
    "cap.set(cv2.CAP_PROP_EXPOSURE, 40)\n",
    "cap.set(cv2.CAP_PROP_AUTO_WB, 0) # Disable automatic white balance (Not sure if this is working)\n",
    "\n",
    "# Create Tracker Object\n",
    "tracker = track.EuclideanDistTracker()\n",
    "\n",
    "# Create BG Subtraction Function with set parameters\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows=False)\n",
    "fgbg.setVarThreshold(100) #The threshold of what is detected as not background (150 can detect regular pieces, but not white)\n",
    "\n",
    "# Create Window\n",
    "cv2.startWindowThread()\n",
    "\n",
    "while(True):\n",
    "    # print(\"in loop\")\n",
    "    # Capture video frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # 0: Pre-Process Image\n",
    "    window_factor = 1      # Size factor of display window\n",
    "    x_left_crop = 0        # Left side window crop\n",
    "    x_right_crop = 1920     # Right side window crop\n",
    "    y_top_crop = 0          # Top side window crop\n",
    "    y_bottom_crop = 1080    # Bottom side window crop\n",
    "    saturation_factor = 1.2 # Factor to saturate image by\n",
    "    value_factor = 1.2      # Factor to increase value of image by\n",
    "\n",
    "    # Crop and adjust saturation of image. Blur to reduce noise from saturation\n",
    "    #frame = frame[y_top_crop:y_bottom_crop, x_left_crop:x_right_crop]\n",
    "    adjusted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(\"float32\")\n",
    "    (h, s, v) = cv2.split(adjusted)\n",
    "    s = s * saturation_factor\n",
    "    s = np.clip(s,0,255)\n",
    "    v = v * value_factor\n",
    "    v = np.clip(v,0,255)\n",
    "    adjusted = cv2.merge([h,s,v])\n",
    "    frame = cv2.cvtColor(adjusted.astype(\"uint8\"), cv2.COLOR_HSV2BGR)\n",
    "    #frame = cv2.GaussianBlur(frame, (25, 25), 0)\n",
    "\n",
    "    # Apply MOG2 to Frame\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Adjust size\n",
    "    #frame = cv2.resize(frame, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "    #fgmask = cv2.resize(fgmask, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    \n",
    "    # 1: Object Detection\n",
    "    min_area = 300 # Min area of contour to detect as piece\n",
    "    \n",
    "    # Iterate through discovered contours and add detections\n",
    "    contours, hierarchy = cv2.findContours(image=fgmask, mode=cv2.RETR_CCOMP, method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "    detections = []\n",
    "    image_copy = frame.copy()\n",
    "    contourFrame = frame.copy()\n",
    "    \n",
    "    cv2.drawContours(contourFrame, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    for i, contour in enumerate(contours):\n",
    "        \n",
    "        if hierarchy[0,i,3] == -1:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            # If contour overlap with border then skip\n",
    "            if((x == 0) or (y == 0) or (x+w >= frame.shape[1]) or (y+h >= frame.shape[0])):\n",
    "                continue\n",
    "            \n",
    "            area = cv2.contourArea(contour)\n",
    "            if(area > min_area):\n",
    "                cv2.putText(contourFrame, \"Area: \" + str(area), (x, y-10), 0,0.5, (255,0,0), 2)\n",
    "                detections.append([x, y, w, h])\n",
    "                \n",
    "    # In order to detect white pieces, we need to write a function that takes all the contours, looks at the ones that make the size criteria, and then\n",
    "    # finds all nearby contours and adds them to the contour space. The white pieces being detected as a bunch of small contours\n",
    "    \n",
    "    # 2: Object Tracking\n",
    "    min_age = 30 # Min age of object to classify as brick\n",
    "    interval = 1 # Frequency of images to save after being classified as brick\n",
    "    \n",
    "    box_ids = tracker.update(detections)\n",
    "    for box_id in box_ids:\n",
    "        x, y, w, h, id, age, uniqueId = box_id\n",
    "        # If a box is \"old enough\" box is determined to be a piece --> take pictures\n",
    "        if age > min_age:\n",
    "            larger = w\n",
    "            if(h > w):\n",
    "                larger = h\n",
    "            cv2.rectangle(image_copy, (x,y), (x+larger, y+larger), (0, 255, 0), 3)\n",
    "            cv2.putText(image_copy, \"PIECE IS HERE\" + \" time:\" + str(age) + \" id:\" + str(uniqueId),(x,y-10),0,0.5,(255, 0, 0), 2)\n",
    "            \n",
    "            # Save bounding box as image every X frames\n",
    "            if x and y > 10:\n",
    "                padding = 10\n",
    "            else:\n",
    "                padding = 0\n",
    "            roi = frame[y:y+larger, x:x+larger]\n",
    "            true_age = age - min_age\n",
    "            \n",
    "            if(true_age % interval == 0):\n",
    "                print(true_age)\n",
    "                print(\"save image\")\n",
    "                cv2.imwrite(str(uniqueId) + \"_\" + str(age) + \".png\", roi)\n",
    "            \n",
    "    \n",
    "    cv2.imshow('Feed with bounding boxes', image_copy)\n",
    "    #cv2.imshow('MOG2 Output', fgmask)\n",
    "    #cv2.imshow('Contours', contourFrame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f5a3809",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# pieceNum = \"3021\"\n",
    "# os.chdir(pieceNum)\n",
    "# Open Video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create Tracker Object\n",
    "tracker = track.EuclideanDistTracker()\n",
    "\n",
    "# Create BG Subtraction Function with set parameters\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows=False)\n",
    "\n",
    "fgbg.setVarThreshold(75) #The threshold of what is detected as not background (150 can detect regular pieces, but not white)\n",
    "\n",
    "\n",
    "# Create Window\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# FOR TRAINING:\n",
    "num_img = 0\n",
    "\n",
    "while(True):\n",
    "    # print(\"in loop\")\n",
    "    # Capture video frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    # 0: Pre-Process Image\n",
    "    window_factor = 1      # Size factor of display window\n",
    "    x_left_crop = 0        # Left side window crop\n",
    "    x_right_crop = 1920     # Right side window crop\n",
    "    y_top_crop = 0          # Top side window crop\n",
    "    y_bottom_crop = 1080    # Bottom side window crop\n",
    "    saturation_factor = 6 # Factor to saturate image by\n",
    "    value_factor = 1.2      # Factor to increase value of image by\n",
    "\n",
    "    # Crop and adjust saturation of image. Blur to reduce noise from saturation\n",
    "    frame = frame[y_top_crop:y_bottom_crop, x_left_crop:x_right_crop]\n",
    "    adjusted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(\"float32\")\n",
    "    (h, s, v) = cv2.split(adjusted)\n",
    "    s = s * saturation_factor\n",
    "    s = np.clip(s,0,255)\n",
    "    v = v * value_factor\n",
    "    v = np.clip(v,0,255)\n",
    "    adjusted = cv2.merge([h,s,v])\n",
    "    frame = cv2.cvtColor(adjusted.astype(\"uint8\"), cv2.COLOR_HSV2BGR)\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "    # Apply MOG2 to Frame\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Adjust size\n",
    "    frame = cv2.resize(frame, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "    fgmask = cv2.resize(fgmask, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    \n",
    "    # 1: Object Detection\n",
    "    min_area = 300 # Min area of contour to detect as piece\n",
    "    \n",
    "    # Iterate through discovered contours and add detections\n",
    "    contours, hierarchy = cv2.findContours(image=fgmask, mode=cv2.RETR_CCOMP, method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "    detections = []\n",
    "    image_copy = frame.copy()\n",
    "    \n",
    "    cv2.drawContours(image_copy, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            # If contour overlap with border then skip\n",
    "            if((x == 0) or (y == 0) or (x+w >= frame.shape[1]) or (y+h >= frame.shape[0])):\n",
    "                continue\n",
    "            \n",
    "            area = cv2.contourArea(contour)\n",
    "            if(area > min_area):\n",
    "                detections.append([x, y, w, h])\n",
    "                \n",
    "    # In order to detect white pieces, we need to write a function that takes all the contours, looks at the ones that make the size criteria, and then\n",
    "    # finds all nearby contours and adds them to the contour space. The white pieces being detected as a bunch of small contours\n",
    "    \n",
    "    # 2: Object Tracking\n",
    "    min_age = 1 # Min age of object to classify as brick\n",
    "    interval = 1 # Frequency of images to save after being classified as brick\n",
    "    \n",
    "    box_ids = tracker.update(detections)\n",
    "    for box_id in box_ids:\n",
    "        x, y, w, h, id, age, uniqueId = box_id\n",
    "        # If a box is \"old enough\" box is determined to be a piece --> take pictures\n",
    "        if age > min_age:\n",
    "            num_img += 1\n",
    "            larger = w\n",
    "            if(h > w):\n",
    "                larger = h\n",
    "            cv2.rectangle(image_copy, (x,y), (x+larger, y+larger), (0, 255, 0), 3)\n",
    "            cv2.putText(image_copy, \"PIECE IS HERE\" + \" time:\" + str(age) + \" id:\" + str(uniqueId),(x,y-10),0,0.5,(255, 0, 0), 2)\n",
    "            \n",
    "            # Save bounding box as image every X frames\n",
    "            roi = frame[y:y+larger, x:x+larger]\n",
    "            #print(str(num_img) + \".jpg\")\n",
    "            true_age = age - min_age\n",
    "            \n",
    "            if(true_age % interval == 0):\n",
    "                print(true_age)\n",
    "                print(\"save image\")\n",
    "                cv2.imwrite(str(uniqueId) + \"_\" + str(num_img) + \".png\", roi)\n",
    "            \n",
    "    \n",
    "    cv2.imshow('Feed with bounding boxes', image_copy)\n",
    "    cv2.imshow('MOG2 Output', fgmask)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eff3aaaf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Open Video\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 3) # auto mode\n",
    "cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1) # manual mode\n",
    "cap.set(cv2.CAP_PROP_EXPOSURE, 50)\n",
    "# Create Window\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# FOR TRAINING:\n",
    "num_img = 0\n",
    "\n",
    "while(True):\n",
    "    # print(\"in loop\")\n",
    "    # Capture video frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('MOG2 Output', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
