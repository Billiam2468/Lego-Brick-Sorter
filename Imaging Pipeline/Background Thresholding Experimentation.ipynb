{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6ec3ea",
   "metadata": {},
   "source": [
    "This is the code that will be imported into our Raspberry Pi to process and send images to our computer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48242ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "# reload our external packages (tracker)\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from skimage import data, filters\n",
    "import tracker as track\n",
    "\n",
    "importlib.reload(track)\n",
    "\n",
    "# Directory to save training images\n",
    "\n",
    "# base_dir = \"/Users/williamlee/Documents/Git Repos/Lego-Brick-Sorter/Imaging Pipeline/testImages\"\n",
    "base_dir = \"/home/billiam/Documents/Lego_Sorter/TestPipelineImages\"\n",
    "os.chdir(base_dir)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d0fb5-6615-4b4c-b50f-e8f8a5d85a2e",
   "metadata": {},
   "source": [
    "**MOG2 APPROACH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9457dc8f-de21-4abf-8c2d-6a06198bae40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"/home/billiam/Documents/Repos/Lego-Brick-Sorter/Imaging Pipeline/white.avi\")\n",
    "frame_counter = 0\n",
    "\n",
    "# Create BG Subtraction Function with set parameters\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows=False)\n",
    "fgbg.setVarThreshold(30) #The threshold of what is detected as not background (150 can detect regular pieces, but not white)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Loop video code\n",
    "    frame_counter += 1\n",
    "    if frame_counter == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        frame_counter = 0 #Or whatever as long as it is the same as next line\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    \n",
    "    # Exit if frame not correctly\n",
    "    if not ret:\n",
    "        print(\"Frame not read correctly. Exiting...\")\n",
    "        break\n",
    "    \n",
    "    #0.Pre-Process Image\n",
    "    saturation_factor = 3 # Factor to saturate image by\n",
    "    value_factor = 1.2     # Factor to increase value of image by\n",
    "\n",
    "    # Crop and adjust saturation of image. Blur to reduce noise from saturation\n",
    "    #frame = frame[y_top_crop:y_bottom_crop, x_left_crop:x_right_crop]\n",
    "    adjusted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(\"float32\")\n",
    "    (h, s, v) = cv2.split(adjusted)\n",
    "    s = s * saturation_factor\n",
    "    s = np.clip(s,0,255)\n",
    "    v = v * value_factor\n",
    "    v = np.clip(v,0,255)\n",
    "    adjusted = cv2.merge([h,s,v])\n",
    "    frame = cv2.cvtColor(adjusted.astype(\"uint8\"), cv2.COLOR_HSV2BGR)\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), cv2.BORDER_DEFAULT)    \n",
    "        \n",
    "    # Apply MOG2 to Frame\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Find contours on MOG2 mask and draw on frame copy\n",
    "    frame_copy = frame.copy()\n",
    "    contours, hierarchy = cv2.findContours(image=fgmask, mode=cv2.RETR_CCOMP, method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(frame_copy, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    window_factor = 0.75\n",
    "    frame = cv2.resize(frame, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "    fgmask = cv2.resize(fgmask, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('MOG2', fgmask)\n",
    "    cv2.imshow('Contours', frame_copy)\n",
    "    if cv2.waitKey(50) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2cb6e3-74e1-405b-88a4-34380d36c6a4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "**DIFFERENCE OF GAUSSIANS APPROACH**\n",
    "\n",
    "Notes:\n",
    "Kind of works! I think I need to work with the thresholding a little bit. So far I've only adjusted the extent threshold.\n",
    "Have to mess with the k and s values for the blurs to maybe get more accurate. Works well with regular colored pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7979af23-9688-40e6-ba50-c10af6c8366a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dog(img, k1, s1, k2, s2):\n",
    "    b1 = cv2.GaussianBlur(img,(k1, k1), s1)\n",
    "    b2 = cv2.GaussianBlur(img,(k2, k2), s2)\n",
    "    return b1 - b2\n",
    "\n",
    "cap = cv2.VideoCapture(\"/home/billiam/Documents/Repos/Lego-Brick-Sorter/Imaging Pipeline/white.avi\")\n",
    "frame_counter = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Loop video code\n",
    "    frame_counter += 1\n",
    "    if frame_counter == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        frame_counter = 0 #Or whatever as long as it is the same as next line\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    \n",
    "    # Color processing:\n",
    "    saturation_factor = 3 # Factor to saturate image by\n",
    "    value_factor = 1      # Factor to increase value of image by\n",
    "    adjusted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(\"float32\")\n",
    "    (h, s, v) = cv2.split(adjusted)\n",
    "    s = s * saturation_factor\n",
    "    s = np.clip(s,0,255)\n",
    "    v = v * value_factor\n",
    "    v = np.clip(v,0,255)\n",
    "    adjusted = cv2.merge([h,s,v])\n",
    "    frame = cv2.cvtColor(adjusted.astype(\"uint8\"), cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    # Exit if frame not correctly\n",
    "    if not ret:\n",
    "        print(\"Frame not read correctly. Exiting...\")\n",
    "        break\n",
    "    \n",
    "    grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    dog_img = dog(grey, 7, 7, 17, 13)\n",
    "    th = cv2.threshold(dog_img ,127,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "    contours, hierarchy = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    frame_copy = frame.copy()\n",
    "    \n",
    "    #cv2.drawContours(frame_copy, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area > 300:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            extent = int(area)/(w*h)\n",
    "            #if extent > 0.1:\n",
    "            rect = cv2.minAreaRect(c)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.intp(box)\n",
    "            cv2.drawContours(frame_copy,[box], 0, (0,255,0), 4)\n",
    "            \n",
    "    \n",
    "    cv2.imshow(\"dog frames\", frame_copy)\n",
    "    cv2.imshow('grayscale', grey)\n",
    "    cv2.imshow(\"dog edges\", dog_img)\n",
    "    \n",
    "    if cv2.waitKey(100) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c86dc-b56f-4c24-b2c9-01f9133fd882",
   "metadata": {},
   "source": [
    "**Sobel and Canny Edge Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "50111e35-3303-49e3-aec7-15ceb4011f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"/home/billiam/Documents/Repos/Lego-Brick-Sorter/Imaging Pipeline/white.avi\")\n",
    "frame_counter = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Loop video code\n",
    "    frame_counter += 1\n",
    "    if frame_counter == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        frame_counter = 0 #Or whatever as long as it is the same as next line\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        \n",
    "    # Color processing:\n",
    "    saturation_factor = 3 # Factor to saturate image by\n",
    "    value_factor = 1      # Factor to increase value of image by\n",
    "    adjusted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(\"float32\")\n",
    "    (h, s, v) = cv2.split(adjusted)\n",
    "    s = s * saturation_factor\n",
    "    s = np.clip(s,0,255)\n",
    "    v = v * value_factor\n",
    "    v = np.clip(v,0,255)\n",
    "    adjusted = cv2.merge([h,s,v])\n",
    "    frame = cv2.cvtColor(adjusted.astype(\"uint8\"), cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    # Exit if frame not correctly\n",
    "    if not ret:\n",
    "        print(\"Frame not read correctly. Exiting...\")\n",
    "        break\n",
    "        \n",
    "    # Grayscale convert\n",
    "    img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Blur img\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "    \n",
    "    # # Sobel Edge Detection\n",
    "    # sobelx = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5) # Sobel Edge Detection on the X axis\n",
    "    # sobely = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5) # Sobel Edge Detection on the Y axis\n",
    "    # sobelxy = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5) # Combined X and Y Sobel Edge Detection\n",
    "    # # Display Sobel Edge Detection Images\n",
    "    # cv2.imshow('Sobel X', sobelx)\n",
    "    # cv2.imshow('Sobel Y', sobely)\n",
    "    # cv2.imshow('Sobel X Y using Sobel() function', sobelxy)\n",
    "    \n",
    "    # Canny Edge Detection\n",
    "    edges = cv2.Canny(image=img_blur, threshold1=0, threshold2=35) # Canny Edge Detection\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    frame_copy = frame.copy()\n",
    "    #cv2.drawContours(frame_copy, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    #cv2.drawContours(frame_copy, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area > 300:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            extent = int(area)/(w*h)\n",
    "            #if extent > 0.1:\n",
    "            rect = cv2.minAreaRect(c)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.intp(box)\n",
    "            cv2.drawContours(frame_copy,[box], 0, (0,255,0), 4)\n",
    "    \n",
    "    # Display Canny Edge Detection Image\n",
    "    cv2.imshow('Canny Edge Detection', edges)\n",
    "    cv2.imshow('Contours', frame_copy)\n",
    "    if cv2.waitKey(100) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f778e8a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "save image\n",
      "2\n",
      "save image\n",
      "3\n",
      "save image\n",
      "4\n",
      "save image\n",
      "5\n",
      "save image\n",
      "6\n",
      "save image\n",
      "7\n",
      "save image\n",
      "1\n",
      "save image\n",
      "2\n",
      "save image\n",
      "3\n",
      "save image\n",
      "4\n",
      "save image\n",
      "1\n",
      "save image\n",
      "2\n",
      "save image\n",
      "3\n",
      "save image\n",
      "1\n",
      "save image\n"
     ]
    }
   ],
   "source": [
    "# pieceNum = \"3021\"\n",
    "# os.chdir(pieceNum)\n",
    "# Open Video\n",
    "#cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(cv2.CAP_V4L2)\n",
    "\n",
    "# Adjust Camera Settings\n",
    "cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1) # Manual exposure mode\n",
    "cap.set(cv2.CAP_PROP_EXPOSURE, 20)\n",
    "cap.set(cv2.CAP_PROP_AUTO_WB, 0) # Disable automatic white balance (Not sure if this is working)\n",
    "\n",
    "# Create Tracker Object\n",
    "tracker = track.EuclideanDistTracker()\n",
    "\n",
    "# Create BG Subtraction Function with set parameters\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows=False)\n",
    "fgbg.setVarThreshold(30) #The threshold of what is detected as not background (150 can detect regular pieces, but not white)\n",
    "\n",
    "# Create Window\n",
    "cv2.startWindowThread()\n",
    "\n",
    "while(True):\n",
    "    # print(\"in loop\")\n",
    "    # Capture video frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # 0: Pre-Process Image\n",
    "    window_factor = 1      # Size factor of display window\n",
    "    x_left_crop = 0        # Left side window crop\n",
    "    x_right_crop = 1920     # Right side window crop\n",
    "    y_top_crop = 0          # Top side window crop\n",
    "    y_bottom_crop = 1080    # Bottom side window crop\n",
    "    saturation_factor = 6 # Factor to saturate image by\n",
    "    value_factor = 1.2      # Factor to increase value of image by\n",
    "\n",
    "    # Crop and adjust saturation of image. Blur to reduce noise from saturation\n",
    "    #frame = frame[y_top_crop:y_bottom_crop, x_left_crop:x_right_crop]\n",
    "    adjusted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(\"float32\")\n",
    "    (h, s, v) = cv2.split(adjusted)\n",
    "    s = s * saturation_factor\n",
    "    s = np.clip(s,0,255)\n",
    "    v = v * value_factor\n",
    "    v = np.clip(v,0,255)\n",
    "    adjusted = cv2.merge([h,s,v])\n",
    "    frame = cv2.cvtColor(adjusted.astype(\"uint8\"), cv2.COLOR_HSV2BGR)\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "    # Apply MOG2 to Frame\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Adjust size\n",
    "    #frame = cv2.resize(frame, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "    #fgmask = cv2.resize(fgmask, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    \n",
    "    # 1: Object Detection\n",
    "    min_area = 300 # Min area of contour to detect as piece\n",
    "    \n",
    "    # Iterate through discovered contours and add detections\n",
    "    contours, hierarchy = cv2.findContours(image=fgmask, mode=cv2.RETR_CCOMP, method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "    detections = []\n",
    "    image_copy = frame.copy()\n",
    "    contourFrame = frame.copy()\n",
    "    \n",
    "    cv2.drawContours(contourFrame, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            # If contour overlap with border then skip\n",
    "            if((x == 0) or (y == 0) or (x+w >= frame.shape[1]) or (y+h >= frame.shape[0])):\n",
    "                continue\n",
    "            \n",
    "            area = cv2.contourArea(contour)\n",
    "            if(area > min_area):\n",
    "                detections.append([x, y, w, h])\n",
    "                \n",
    "    # In order to detect white pieces, we need to write a function that takes all the contours, looks at the ones that make the size criteria, and then\n",
    "    # finds all nearby contours and adds them to the contour space. The white pieces being detected as a bunch of small contours\n",
    "    \n",
    "    # 2: Object Tracking\n",
    "    min_age = 1 # Min age of object to classify as brick\n",
    "    interval = 1 # Frequency of images to save after being classified as brick\n",
    "    \n",
    "    box_ids = tracker.update(detections)\n",
    "    for box_id in box_ids:\n",
    "        x, y, w, h, id, age, uniqueId = box_id\n",
    "        # If a box is \"old enough\" box is determined to be a piece --> take pictures\n",
    "        if age > min_age:\n",
    "            larger = w\n",
    "            if(h > w):\n",
    "                larger = h\n",
    "            cv2.rectangle(image_copy, (x,y), (x+larger, y+larger), (0, 255, 0), 3)\n",
    "            cv2.putText(image_copy, \"PIECE IS HERE\" + \" time:\" + str(age) + \" id:\" + str(uniqueId),(x,y-10),0,0.5,(255, 0, 0), 2)\n",
    "            \n",
    "            # Save bounding box as image every X frames\n",
    "            roi = frame[y:y+larger, x:x+larger]\n",
    "            true_age = age - min_age\n",
    "            \n",
    "            if(true_age % interval == 0):\n",
    "                print(true_age)\n",
    "                print(\"save image\")\n",
    "                cv2.imwrite(str(uniqueId) + \"_\" + str(age) + \".png\", roi)\n",
    "            \n",
    "    \n",
    "    cv2.imshow('Feed with bounding boxes', image_copy)\n",
    "    cv2.imshow('MOG2 Output', fgmask)\n",
    "    cv2.imshow('Contours', contourFrame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f5a3809",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pieceNum = \"3021\"\n",
    "# os.chdir(pieceNum)\n",
    "# Open Video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create Tracker Object\n",
    "tracker = track.EuclideanDistTracker()\n",
    "\n",
    "# Create BG Subtraction Function with set parameters\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows=False)\n",
    "\n",
    "fgbg.setVarThreshold(75) #The threshold of what is detected as not background (150 can detect regular pieces, but not white)\n",
    "\n",
    "\n",
    "# Create Window\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# FOR TRAINING:\n",
    "num_img = 0\n",
    "\n",
    "while(True):\n",
    "    # print(\"in loop\")\n",
    "    # Capture video frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    # 0: Pre-Process Image\n",
    "    window_factor = 1      # Size factor of display window\n",
    "    x_left_crop = 0        # Left side window crop\n",
    "    x_right_crop = 1920     # Right side window crop\n",
    "    y_top_crop = 0          # Top side window crop\n",
    "    y_bottom_crop = 1080    # Bottom side window crop\n",
    "    saturation_factor = 6 # Factor to saturate image by\n",
    "    value_factor = 1.2      # Factor to increase value of image by\n",
    "\n",
    "    # Crop and adjust saturation of image. Blur to reduce noise from saturation\n",
    "    frame = frame[y_top_crop:y_bottom_crop, x_left_crop:x_right_crop]\n",
    "    adjusted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(\"float32\")\n",
    "    (h, s, v) = cv2.split(adjusted)\n",
    "    s = s * saturation_factor\n",
    "    s = np.clip(s,0,255)\n",
    "    v = v * value_factor\n",
    "    v = np.clip(v,0,255)\n",
    "    adjusted = cv2.merge([h,s,v])\n",
    "    frame = cv2.cvtColor(adjusted.astype(\"uint8\"), cv2.COLOR_HSV2BGR)\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "    # Apply MOG2 to Frame\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Adjust size\n",
    "    frame = cv2.resize(frame, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "    fgmask = cv2.resize(fgmask, None, fx=window_factor, fy=window_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    \n",
    "    # 1: Object Detection\n",
    "    min_area = 300 # Min area of contour to detect as piece\n",
    "    \n",
    "    # Iterate through discovered contours and add detections\n",
    "    contours, hierarchy = cv2.findContours(image=fgmask, mode=cv2.RETR_CCOMP, method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "    detections = []\n",
    "    image_copy = frame.copy()\n",
    "    \n",
    "    cv2.drawContours(image_copy, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            # If contour overlap with border then skip\n",
    "            if((x == 0) or (y == 0) or (x+w >= frame.shape[1]) or (y+h >= frame.shape[0])):\n",
    "                continue\n",
    "            \n",
    "            area = cv2.contourArea(contour)\n",
    "            if(area > min_area):\n",
    "                detections.append([x, y, w, h])\n",
    "                \n",
    "    # In order to detect white pieces, we need to write a function that takes all the contours, looks at the ones that make the size criteria, and then\n",
    "    # finds all nearby contours and adds them to the contour space. The white pieces being detected as a bunch of small contours\n",
    "    \n",
    "    # 2: Object Tracking\n",
    "    min_age = 1 # Min age of object to classify as brick\n",
    "    interval = 1 # Frequency of images to save after being classified as brick\n",
    "    \n",
    "    box_ids = tracker.update(detections)\n",
    "    for box_id in box_ids:\n",
    "        x, y, w, h, id, age, uniqueId = box_id\n",
    "        # If a box is \"old enough\" box is determined to be a piece --> take pictures\n",
    "        if age > min_age:\n",
    "            num_img += 1\n",
    "            larger = w\n",
    "            if(h > w):\n",
    "                larger = h\n",
    "            cv2.rectangle(image_copy, (x,y), (x+larger, y+larger), (0, 255, 0), 3)\n",
    "            cv2.putText(image_copy, \"PIECE IS HERE\" + \" time:\" + str(age) + \" id:\" + str(uniqueId),(x,y-10),0,0.5,(255, 0, 0), 2)\n",
    "            \n",
    "            # Save bounding box as image every X frames\n",
    "            roi = frame[y:y+larger, x:x+larger]\n",
    "            #print(str(num_img) + \".jpg\")\n",
    "            true_age = age - min_age\n",
    "            \n",
    "            if(true_age % interval == 0):\n",
    "                print(true_age)\n",
    "                print(\"save image\")\n",
    "                cv2.imwrite(str(uniqueId) + \"_\" + str(num_img) + \".png\", roi)\n",
    "            \n",
    "    \n",
    "    cv2.imshow('Feed with bounding boxes', image_copy)\n",
    "    cv2.imshow('MOG2 Output', fgmask)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eff3aaaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Open Video\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 3) # auto mode\n",
    "cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1) # manual mode\n",
    "cap.set(cv2.CAP_PROP_EXPOSURE, 50)\n",
    "# Create Window\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# FOR TRAINING:\n",
    "num_img = 0\n",
    "\n",
    "while(True):\n",
    "    # print(\"in loop\")\n",
    "    # Capture video frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('MOG2 Output', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
